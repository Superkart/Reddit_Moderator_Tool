{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      subreddit                                              title  \\\n",
      "0     AskReddit  What was your “I’m dating a fucking idiot” mom...   \n",
      "1     AskReddit  What's a story where the \"bad guys\" are actual...   \n",
      "2     AskReddit  What made you lose a significant amount of wei...   \n",
      "3     AskReddit                       What song is torture to you?   \n",
      "4  ChangeMyView  Meta: r/changemyview is recruiting new moderators   \n",
      "\n",
      "                                             content  upvotes  upvote_ratio  \\\n",
      "0                                                NaN     2327          0.93   \n",
      "1                                                NaN    12041          0.94   \n",
      "2                                                NaN      945          0.94   \n",
      "3                                                NaN      489          0.91   \n",
      "4  It's that time of the year folks. We're lookin...        9          0.58   \n",
      "\n",
      "   comments_count            author            timestamp  post_id  \n",
      "0            1584  post-nutclarence  2024-11-30 17:29:46  1h3pfcx  \n",
      "1            7688     WidowofBielsa  2024-11-30 07:19:00  1h3cex6  \n",
      "2            2075        ExilicRose  2024-11-30 18:43:38  1h3qyip  \n",
      "3            2310        KingStevoI  2024-11-30 18:19:26  1h3qgt3  \n",
      "4              34  RedditExplorer89  2024-11-27 23:58:46  1h1pb0h  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"subreddit_posts.csv\")\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subreddit', 'title', 'content', 'upvotes', 'upvote_ratio',\n",
       "       'comments_count', 'author', 'timestamp', 'post_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18049, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AskReddit', 'ChangeMyView', 'TodayILearned', 'self', 'offmychest', 'Showerthoughts', 'personalfinance', 'AskScience', 'Writing', 'Advice', 'LetsNotMeet', 'SelfImprovement', 'DecidingToBeBetter', 'AskHistorians', 'TwoXChromosomes', 'CasualConversation', 'InternetIsBeautiful', 'nosleep', 'WritingPrompts', 'ExplainLikeImFive', 'TrueOffMyChest', 'UnpopularOpinion', 'relationships', 'TrueAskReddit', 'Confession', 'ShortScaryStories', 'ProRevenge', 'NuclearRevenge', 'LifeProTips', 'needadvice', 'TrueUnpopularOpinion']\n"
     ]
    }
   ],
   "source": [
    "unique_subreddits_list = df['subreddit'].unique().tolist()\n",
    "print(unique_subreddits_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subreddit: 0 null values\n",
      "title: 0 null values\n",
      "content: 3043 null values\n",
      "upvotes: 0 null values\n",
      "upvote_ratio: 0 null values\n",
      "comments_count: 0 null values\n",
      "author: 0 null values\n",
      "timestamp: 0 null values\n",
      "post_id: 0 null values\n"
     ]
    }
   ],
   "source": [
    "# Get the count of null values in each column and print with column names\n",
    "null_values = df.isnull().sum()\n",
    "\n",
    "# Display column names with their null count\n",
    "for column, count in null_values.items():\n",
    "    print(f\"{column}: {count} null values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where 'content' column has null values\n",
    "df = df.dropna(subset=['content'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subreddit: 0 null values\n",
      "title: 0 null values\n",
      "content: 0 null values\n",
      "upvotes: 0 null values\n",
      "upvote_ratio: 0 null values\n",
      "comments_count: 0 null values\n",
      "author: 0 null values\n",
      "timestamp: 0 null values\n",
      "post_id: 0 null values\n"
     ]
    }
   ],
   "source": [
    "null_values = df.isnull().sum()\n",
    "\n",
    "# Display column names with their null count\n",
    "for column, count in null_values.items():\n",
    "    print(f\"{column}: {count} null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15006, 9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you already have your DataFrame 'df'\n",
    "# Sample DataFrame with the given columns\n",
    "# df = pd.read_csv(\"your_existing_file.csv\")  # Replace with your actual DataFrame\n",
    "\n",
    "# Convert 'timestamp' to datetime with automatic format inference\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "\n",
    "# Extract Date, Time, and AM/PM\n",
    "df['date'] = df['timestamp'].dt.date\n",
    "df['time'] = df['timestamp'].dt.strftime('%I:%M')  # Time in the format \"HH:MM AM/PM\"\n",
    "df['AM_PM'] = df['timestamp'].dt.strftime('%p')  # Extract AM or PM\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15006, 12)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        subreddit                                              title  \\\n",
      "0       AskReddit                2024 United States Elections Thread   \n",
      "858  ChangeMyView  Meta: Research Collaboration Opportunity with ...   \n",
      "859  ChangeMyView  CMV: Goodhearted \"cultural appropriation\" is f...   \n",
      "860  ChangeMyView  CMV: Anyone given a life sentence should also ...   \n",
      "861  ChangeMyView  CMV: Most people are too lazy to actually seek...   \n",
      "\n",
      "                                               content  upvotes  upvote_ratio  \\\n",
      "0    Please use this thread to discuss the ongoing ...      122          0.70   \n",
      "858  From time to time, CMV will partner with profe...        7          0.71   \n",
      "859  I am Austrian and when non-Austrians find a li...      132          0.85   \n",
      "860  Anyone given a life sentence should also be gi...       45          0.84   \n",
      "861  I always see people online talking about how t...      123          0.84   \n",
      "\n",
      "     comments_count               author           timestamp  post_id  \\\n",
      "0              7415  AskRedditModerators 2024-11-05 17:14:10  1gkk9s3   \n",
      "858               9              Ansuz07 2024-10-30 10:42:19  1gfpl4m   \n",
      "859              69  Possible_Lemon_9527 2024-11-14 19:30:43  1grkyyp   \n",
      "860              38     Christ-The-Slave 2024-11-14 19:39:44  1grl5em   \n",
      "861              61   Neckties-Over-Bows 2024-11-14 12:39:25  1grbu7n   \n",
      "\n",
      "           date   time AM_PM  upvote_ratio_normalized  \\\n",
      "0    2024-11-05  05:14    PM                 0.684211   \n",
      "858  2024-10-30  10:42    AM                 0.694737   \n",
      "859  2024-11-14  07:30    PM                 0.842105   \n",
      "860  2024-11-14  07:39    PM                 0.831579   \n",
      "861  2024-11-14  12:39    PM                 0.831579   \n",
      "\n",
      "     comments_count_normalized  upvotes_normalized  \n",
      "0                     0.443322            0.004664  \n",
      "858                   0.000538            0.000268  \n",
      "859                   0.004125            0.005046  \n",
      "860                   0.002272            0.001720  \n",
      "861                   0.003647            0.004702  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Assuming your dataframe 'df' is already loaded with the necessary columns\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Select the columns to normalize\n",
    "columns_to_normalize = ['upvote_ratio', 'comments_count', 'upvotes']\n",
    "\n",
    "# Apply the scaler to the selected columns and create new columns\n",
    "df[['upvote_ratio_normalized', 'comments_count_normalized', 'upvotes_normalized']] = scaler.fit_transform(df[columns_to_normalize])\n",
    "\n",
    "# Display the updated DataFrame with the new normalized columns\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'author', 'timestamp', and 'post_id' columns\n",
    "# df = df.drop(columns=['author', 'timestamp', 'post_id'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'] = df['title'].str.replace(r'\\W', ' ').str.lower()\n",
    "df['content'] = df['content'].str.replace(r'\\W', ' ').str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tokenize import word_tokenize\n",
    "# df['title_tokens'] = df['title'].apply(word_tokenize)\n",
    "# df['content_tokens'] = df['content'].apply(word_tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15006, 15)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('cleaned_labeled_subreddit_posts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[df['upvotes'] < df['upvotes'].quantile(0.95)]  # Removing top 5% of extreme values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the entire DataFrame to JSON format and save it to a file\n",
    "# df.to_json(\"data.json\", orient='records', lines=True)\n",
    "\n",
    "# # Notify the user that the file has been saved\n",
    "# print(\"Data has been saved to 'data.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned and merged data saved to cleaned_merged_comments.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to clean and group the comments in a dataframe\n",
    "def clean_and_group(df, group_by_column, comment_column='comment', author_column='comment_author'):\n",
    "    # Step 1: Remove rows where 'comment' is '[deleted]' or 'comment_author' is 'auto_moderator'\n",
    "    df_cleaned = df[(df[comment_column] != '[deleted]') & (df[author_column] != 'auto_moderator')]\n",
    "\n",
    "    # Step 2: Define aggregation dynamically\n",
    "    aggregation_dict = {\n",
    "        comment_column: lambda x: ' '.join(x),  # Combine comments into one\n",
    "    }\n",
    "    # Add optional fields if they exist and aren't the grouping column\n",
    "    optional_fields = ['post_content', 'post_upvotes', 'timestamp', 'post_id', 'post_title']\n",
    "    for field in optional_fields:\n",
    "        if field in df_cleaned.columns and field != group_by_column:\n",
    "            aggregation_dict[field] = 'first'\n",
    "\n",
    "    # Group and reset the index\n",
    "    grouped_df = df_cleaned.groupby(group_by_column).agg(aggregation_dict).reset_index()\n",
    "    return grouped_df\n",
    "\n",
    "# File paths\n",
    "file_path_1 = 'top_level_comments.csv'\n",
    "file_path_2 = 'top_level_comments_01.csv'\n",
    "post_metadata_path = 'cleaned_labeled_subreddit_posts.csv'  # File containing post metadata\n",
    "\n",
    "# Load the CSV files\n",
    "df1 = pd.read_csv(file_path_1, on_bad_lines='skip')  # Skipping bad lines in case there are any\n",
    "df2 = pd.read_csv(file_path_2, on_bad_lines='skip')  # Skipping bad lines in case there are any\n",
    "post_metadata = pd.read_csv(post_metadata_path)\n",
    "\n",
    "# Clean and group each dataframe\n",
    "df1_cleaned = clean_and_group(df1, group_by_column='post_id')  # Group by post_id in the first file\n",
    "df2_cleaned = clean_and_group(df2, group_by_column='post_title')  # Group by post_title in the second file\n",
    "\n",
    "# Step 1: Concatenate the cleaned dataframes\n",
    "final_df = pd.concat([df1_cleaned, df2_cleaned], ignore_index=True)\n",
    "\n",
    "# Step 2: Merge the `upvote_ratio` column from `post_metadata` based on `post_id`\n",
    "final_df = final_df.merge(\n",
    "    post_metadata[['post_id', 'upvote_ratio']],\n",
    "    on='post_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Save the final cleaned dataframe to a new CSV file\n",
    "output_file_path = 'cleaned_merged_comments.csv'\n",
    "final_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Cleaned and merged data saved to {output_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
