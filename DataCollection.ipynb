{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAH770GMcFcy"
   },
   "source": [
    "## Project Overview\n",
    "\n",
    "### Problem Statement\n",
    "In today's digital world, understanding how Reddit communities function is crucial for moderators, users, and researchers...\n",
    "\n",
    "### Data Collection Overview\n",
    "- **Tools**:\n",
    "  - PRAW (Python Reddit API Wrapper)\n",
    "  - BeautifulSoup / Scrapy\n",
    "  \n",
    "- **Data Points to Collect**:\n",
    "  - **Posts**: Title, content, upvotes...\n",
    "  - **Comments**: Content, upvotes...\n",
    "  \n",
    "### Solution Approach\n",
    "1. **Sentiment & Engagement Analysis**\n",
    "   - Visualizations using Matplotlib and Seaborn...\n",
    "   \n",
    "2. **Correlation Analysis**\n",
    "   - Apply classification algorithms using scikit-learn...\n",
    "\n",
    "### Expected Deliverables\n",
    "- Insight Report\n",
    "- Actionable Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1xAJ3tSeEx6"
   },
   "source": [
    "## Setup Environment\n",
    "\n",
    "### Purpose\n",
    "This section prepares our Google Colab environment for the Reddit Communities analysis project as outlined in our team's proposal. We'll install the necessary Python libraries to handle data collection, processing, analysis, and visualization.\n",
    "\n",
    "### Key Libraries\n",
    "- PRAW: For accessing the Reddit API\n",
    "- pandas: For data manipulation and analysis\n",
    "- numpy: For numerical computing\n",
    "- matplotlib and seaborn: For data visualization\n",
    "- nltk: For natural language processing and sentiment analysis\n",
    "- scikit-learn: For machine learning tasks\n",
    "\n",
    "### Alignment with Project Goals\n",
    "These libraries support our objectives of:\n",
    "1. Analyzing moderation strategies\n",
    "2. Predicting post impact\n",
    "3. Visualizing Reddit community interactions\n",
    "\n",
    "### Installation Code\n",
    "Run the following cell to install the required libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MCitsxPEa-Kb",
    "outputId": "5c380300-ea61-4a94-e37a-c18e3f3d1fe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in d:\\uicsem1\\introtodatascience\\class-project-cheesy-little-explorers\\venv\\lib\\site-packages (7.8.1)\n",
      "Requirement already satisfied: prawcore<3,>=2.4 in d:\\uicsem1\\introtodatascience\\class-project-cheesy-little-explorers\\venv\\lib\\site-packages (from praw) (2.4.0)\n",
      "Requirement already satisfied: update_checker>=0.18 in d:\\uicsem1\\introtodatascience\\class-project-cheesy-little-explorers\\venv\\lib\\site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in d:\\uicsem1\\introtodatascience\\class-project-cheesy-little-explorers\\venv\\lib\\site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in d:\\uicsem1\\introtodatascience\\class-project-cheesy-little-explorers\\venv\\lib\\site-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\uicsem1\\introtodatascience\\class-project-cheesy-little-explorers\\venv\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\uicsem1\\introtodatascience\\class-project-cheesy-little-explorers\\venv\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\uicsem1\\introtodatascience\\class-project-cheesy-little-explorers\\venv\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\uicsem1\\introtodatascience\\class-project-cheesy-little-explorers\\venv\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import seaborn\n",
    "import nltk\n",
    "%pip install praw\n",
    "import praw\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Praw API Config\n",
    "\n",
    "This block of code create tje connection with the reddit application and by authenticating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "science\n",
      "Reddit Science\n"
     ]
    }
   ],
   "source": [
    "def setup_reddit_api():\n",
    "    return praw.Reddit(\n",
    "        client_id=\"nrakGjG_wnBE_5UdcHNJoQ\",\n",
    "        client_secret=\"qmGr1q_4pGIBR0pYJE8cyhUbTbdX2w\",\n",
    "        user_agent=\"LittleCheesyExplorers/1.0 (Reddit Communities Analysis Project)\"\n",
    "    )\n",
    "\n",
    "reddit = setup_reddit_api()\n",
    "\n",
    "print(reddit.user.me())\n",
    "subreddit = reddit.subreddit(\"science\")\n",
    "print(subreddit.display_name)\n",
    "print(subreddit.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read required Subreddits \n",
    "This code block will store the names of the subreddits that we want to collect data from.\n",
    "\n",
    "The names will be stored in a text file and we will read from that and scrape based on that list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AskReddit', 'news', 'funny', 'gaming', 'todayilearned', 'science']\n"
     ]
    }
   ],
   "source": [
    "# Read subreddit names from the text file\n",
    "with open('subreddits.txt', 'r') as file:\n",
    "    subreddit_list = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "print(subreddit_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collection Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_subreddit_posts(subreddit_name, post_limit=10):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts_data = []\n",
    "\n",
    "    for post in subreddit.hot(limit=post_limit):\n",
    "        posts_data.append({\n",
    "            'subreddit': subreddit_name,\n",
    "            'title': post.title,\n",
    "            'content': post.selftext,\n",
    "            'upvotes': post.score,\n",
    "            'upvote_ratio': post.upvote_ratio,\n",
    "            'comments_count': post.num_comments,\n",
    "            'author': post.author.name if post.author else '[deleted]',\n",
    "            'timestamp': datetime.fromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'post_id': post.id\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(posts_data)\n",
    "\n",
    "def collect_posts_from_subreddits(subreddit_list, post_limit=10): ## RUNS ON MANY SUBREDDITS(A list of subreddits)\n",
    "    all_posts = []\n",
    "    \n",
    "    for subreddit_name in subreddit_list:\n",
    "        print(f\"Collecting posts from r/{subreddit_name}\")\n",
    "        try:\n",
    "            posts_df = collect_subreddit_posts(subreddit_name, post_limit)\n",
    "            all_posts.append(posts_df)\n",
    "            print(f\"Collected {len(posts_df)} posts from r/{subreddit_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error collecting posts from r/{subreddit_name}: {str(e)}\")\n",
    "    \n",
    "    combined_df = pd.concat(all_posts, ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "def collect_subreddit_level_data(reddit, subreddits, limit=10):\n",
    "    subreddit_level_data = []\n",
    "\n",
    "    for subreddit_name in subreddits[:limit]:\n",
    "        try:\n",
    "            subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "            subscriber_count = subreddit.subscribers\n",
    "\n",
    "            try:\n",
    "                rules = list(subreddit.rules())\n",
    "                num_rules = len(rules)\n",
    "                rule_severity = [rule.severity for rule in rules]\n",
    "            except Exception as rule_error:\n",
    "                num_rules = 0\n",
    "                rule_severity = []\n",
    "                print(f\"Could not fetch rules for r/{subreddit_name}: {rule_error}\")\n",
    "\n",
    "            try:\n",
    "                moderators = len(list(subreddit.moderators()))\n",
    "            except Exception as mod_error:\n",
    "                moderators = 0\n",
    "                print(f\"Could not fetch moderators for r/{subreddit_name}: {mod_error}\")\n",
    "\n",
    "            subreddit_data = {\n",
    "                \"subreddit_name\": subreddit_name,\n",
    "                \"subscriber_count\": subscriber_count,\n",
    "                \"num_rules\": num_rules,\n",
    "                \"moderator_count\": moderators,\n",
    "                \"rule_severity\": rule_severity\n",
    "            }\n",
    "            subreddit_level_data.append(subreddit_data)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for subreddit {subreddit_name}: {e}\")\n",
    "\n",
    "    return subreddit_level_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Collection Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting posts from r/AskReddit\n",
      "Collected 860 posts from r/AskReddit\n",
      "Collecting posts from r/news\n",
      "Collected 202 posts from r/news\n",
      "Collecting posts from r/funny\n",
      "Collected 373 posts from r/funny\n",
      "Collecting posts from r/gaming\n",
      "Collected 151 posts from r/gaming\n",
      "Collecting posts from r/todayilearned\n",
      "Collected 502 posts from r/todayilearned\n",
      "Collecting posts from r/science\n",
      "Collected 770 posts from r/science\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Local\\Temp\\ipykernel_19160\\3314604434.py:43: DeprecationWarning: Calling SubredditRules to get a list of rules is deprecated. Remove the parentheses to use the iterator. View the PRAW documentation on how to change the code in order to use the iterator (https://praw.readthedocs.io/en/latest/code_overview/other/subredditrules.html#praw.models.reddit.rules.SubredditRules.__call__).\n",
      "  num_rules = len(subreddit.rules())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for subreddit AskReddit: received 403 HTTP response\n",
      "Error fetching data for subreddit news: received 403 HTTP response\n",
      "Error fetching data for subreddit funny: received 403 HTTP response\n",
      "Error fetching data for subreddit gaming: received 403 HTTP response\n",
      "Error fetching data for subreddit todayilearned: received 403 HTTP response\n",
      "Error fetching data for subreddit science: received 403 HTTP response\n",
      "Data has been written to subredditLevel_Data.csv\n",
      "\n",
      "All posts data saved to subreddit_posts.csv\n",
      "\n",
      "Summary of collected posts:\n",
      "subreddit\n",
      "AskReddit        860\n",
      "science          770\n",
      "todayilearned    502\n",
      "funny            373\n",
      "news             202\n",
      "gaming           151\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample post data:\n",
      "subreddit                                                 AskReddit\n",
      "title                           2024 United States Elections Thread\n",
      "content           Please use this thread to discuss the ongoing ...\n",
      "upvotes                                                         103\n",
      "upvote_ratio                                                   0.68\n",
      "comments_count                                                 6907\n",
      "author                                          AskRedditModerators\n",
      "timestamp                                       2024-11-05 17:14:10\n",
      "post_id                                                     1gkk9s3\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Collect posts from all subreddits\n",
    "all_posts_df = collect_posts_from_subreddits(subreddit_list, post_limit=10000)\n",
    "\n",
    "collect_subreddit_level_data(reddit, subreddit_list, limit=5000)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Save all collected posts to a single CSV file\n",
    "csv_filename = \"subreddit_posts.csv\"\n",
    "all_posts_df.to_csv(csv_filename, index=False)\n",
    "print(f\"\\nAll posts data saved to {csv_filename}\")\n",
    "\n",
    "# Print summary of collected posts\n",
    "print(\"\\nSummary of collected posts:\")\n",
    "print(all_posts_df['subreddit'].value_counts())\n",
    "\n",
    "# Sample data output\n",
    "print(\"\\nSample post data:\")\n",
    "print(all_posts_df.iloc[0] if len(all_posts_df) > 0 else \"No posts collected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New features added and file saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# adding derived columns \n",
    "\n",
    "#import pandas as pd\n",
    "#from datetime import datetime\n",
    "\n",
    "# Load the existing CSV file\n",
    "df = pd.read_csv('subreddit_posts.csv')\n",
    "\n",
    "# Calculate Title Length\n",
    "df['title_length'] = df['title'].apply(len)\n",
    "\n",
    "# Calculate Post Length (use content column, assuming it's a text-based post)\n",
    "df['post_length'] = df['content'].apply(lambda x: len(str(x)) if pd.notnull(x) else 0)\n",
    "\n",
    "# Convert the timestamp to  datetime format\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "\n",
    "# Calculate Time of Day (hour of the day)\n",
    "df['time_of_day'] = df['timestamp'].dt.hour\n",
    "\n",
    "# Calculate Day of the Week\n",
    "df['day_of_week'] = df['timestamp'].dt.day_name()\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv('subreddit_posts.csv', index=False)\n",
    "\n",
    "print(\"New features added and file saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV Creation and Storing\n",
    "In this Code Block we will be creating a csv file and storing all our scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
