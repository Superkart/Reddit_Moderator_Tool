{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAH770GMcFcy"
   },
   "source": [
    "## Project Overview\n",
    "\n",
    "### Problem Statement\n",
    "In today's digital world, understanding how Reddit communities function is crucial for moderators, users, and researchers...\n",
    "\n",
    "### Data Collection Overview\n",
    "- **Tools**:\n",
    "  - PRAW (Python Reddit API Wrapper)\n",
    "  - BeautifulSoup / Scrapy\n",
    "  \n",
    "- **Data Points to Collect**:\n",
    "  - **Posts**: Title, content, upvotes...\n",
    "  - **Comments**: Content, upvotes...\n",
    "  \n",
    "### Solution Approach\n",
    "1. **Sentiment & Engagement Analysis**\n",
    "   - Visualizations using Matplotlib and Seaborn...\n",
    "   \n",
    "2. **Correlation Analysis**\n",
    "   - Apply classification algorithms using scikit-learn...\n",
    "\n",
    "### Expected Deliverables\n",
    "- Insight Report\n",
    "- Actionable Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1xAJ3tSeEx6"
   },
   "source": [
    "## Setup Environment\n",
    "\n",
    "### Purpose\n",
    "This section prepares our Google Colab environment for the Reddit Communities analysis project as outlined in our team's proposal. We'll install the necessary Python libraries to handle data collection, processing, analysis, and visualization.\n",
    "\n",
    "### Key Libraries\n",
    "- PRAW: For accessing the Reddit API\n",
    "- pandas: For data manipulation and analysis\n",
    "- numpy: For numerical computing\n",
    "- matplotlib and seaborn: For data visualization\n",
    "- nltk: For natural language processing and sentiment analysis\n",
    "- scikit-learn: For machine learning tasks\n",
    "\n",
    "### Alignment with Project Goals\n",
    "These libraries support our objectives of:\n",
    "1. Analyzing moderation strategies\n",
    "2. Predicting post impact\n",
    "3. Visualizing Reddit community interactions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup and Installation\n",
    "Make sure to install all necessary libraries first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MCitsxPEa-Kb",
    "outputId": "5c380300-ea61-4a94-e37a-c18e3f3d1fe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in c:\\users\\karth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (7.8.1)\n",
      "Requirement already satisfied: prawcore<3,>=2.4 in c:\\users\\karth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from praw) (2.4.0)\n",
      "Requirement already satisfied: update_checker>=0.18 in c:\\users\\karth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\karth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\karth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\karth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\karth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\karth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\karth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install praw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import praw  \n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Initialize Reddit API\n",
    "Define and call a function to authenticate with the Reddit API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "def setup_reddit_api():\n",
    "    return praw.Reddit(\n",
    "        client_id=\"nrakGjG_wnBE_5UdcHNJoQ\",\n",
    "        client_secret=\"qmGr1q_4pGIBR0pYJE8cyhUbTbdX2w\",\n",
    "        user_agent=\"LittleCheesyExplorers/1.0 (Reddit Communities Analysis Project)\"\n",
    "    )\n",
    "\n",
    "reddit = setup_reddit_api()\n",
    "\n",
    "print(reddit.user.me())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Load Read required Subreddits \n",
    "This code block will store the names of the subreddits that we want to collect data from.\n",
    "\n",
    "The names will be stored in a text file and we will read from that and scrape based on that list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WritingPrompts', 'TrueOffMyChest', 'NoSleep', 'ExplainLikeImFive', 'IAmA', 'CasualConversation', 'TrueAskReddit', 'Confession', 'relationships', 'ShortScaryStories', 'ProRevenge', 'NuclearRevenge', 'LifeProTips', 'needadvice', 'TrueUnpopularOpinion']\n"
     ]
    }
   ],
   "source": [
    "with open('subreddits.txt', 'r') as file:\n",
    "    subreddit_list = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "print(subreddit_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Functions to Collect Data from Reddit\n",
    "Define functions to collect posts, comments, and subreddit-level data. This is separated for modularity and ease of testing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Collect Posts from a Subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_subreddit_posts(subreddit_name, post_limit=10):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts_data = []\n",
    "\n",
    "    for post in subreddit.hot(limit=post_limit):\n",
    "        posts_data.append({\n",
    "            'subreddit': subreddit_name,\n",
    "            'title': post.title,\n",
    "            'content': post.selftext,\n",
    "            'upvotes': post.score,\n",
    "            'upvote_ratio': post.upvote_ratio,\n",
    "            'comments_count': post.num_comments,\n",
    "            'author': post.author.name if post.author else '[deleted]',\n",
    "            'timestamp': datetime.fromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'post_id': post.id\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(posts_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Collect Data from Multiple Subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_posts_from_subreddits(subreddit_list, post_limit=10):\n",
    "    all_posts = []\n",
    "\n",
    "    for subreddit_name in subreddit_list:\n",
    "        print(f\"Collecting posts from r/{subreddit_name}\")\n",
    "        try:\n",
    "            posts_df = collect_subreddit_posts(subreddit_name, post_limit)\n",
    "            all_posts.append(posts_df)\n",
    "            print(f\"Collected {len(posts_df)} posts from r/{subreddit_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error collecting posts from r/{subreddit_name}: {str(e)}\")\n",
    "\n",
    "    combined_df = pd.concat(all_posts, ignore_index=True)\n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Collect Subreddit-Level Data (Moderators, Rules, Subscriber Counts)\n",
    "Functions to collect metadata for each subreddit, including subscriber count, rules, and moderator counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_subreddit_level_data(reddit, subreddits, limit=10): \n",
    "    subreddit_level_data = []\n",
    "\n",
    "    for subreddit_name in subreddits[:limit]:\n",
    "        try:\n",
    "            subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "            subscriber_count = subreddit.subscribers\n",
    "\n",
    "            try:\n",
    "                rules = list(subreddit.rules())\n",
    "                num_rules = len(rules)\n",
    "                rule_severity = [rule.severity for rule in rules]\n",
    "            except Exception as rule_error:\n",
    "                num_rules = 0\n",
    "                rule_severity = []\n",
    "                print(f\"Could not fetch rules for r/{subreddit_name}: {rule_error}\")\n",
    "\n",
    "            try:\n",
    "                moderators = len(list(subreddit.moderators()))\n",
    "            except Exception as mod_error:\n",
    "                moderators = 0\n",
    "                print(f\"Could not fetch moderators for r/{subreddit_name}: {mod_error}\")\n",
    "\n",
    "            subreddit_data = {\n",
    "                \"subreddit_name\": subreddit_name,\n",
    "                \"subscriber_count\": subscriber_count,\n",
    "                \"num_rules\": num_rules,\n",
    "                \"moderator_count\": moderators,\n",
    "                \"rule_severity\": rule_severity\n",
    "            }\n",
    "            subreddit_level_data.append(subreddit_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for subreddit {subreddit_name}: {e}\")\n",
    "\n",
    "    return subreddit_level_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Comments method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    # Read the post IDs and other data from the provided CSV file\\n    posts_df = pd.read_csv(csv_file)\\n    comments_data = []\\n    \\n    for _, row in posts_df.iterrows():\\n        post_id = row[\\'post_id\\']\\n        subreddit_name = row[\\'subreddit\\']\\n        \\n        try:\\n            # Fetch the post using the post_id\\n            post = reddit.submission(id=post_id)\\n            \\n            # Replace \"MoreComments\" with empty (expand all top-level comments)\\n            post.comments.replace_more(limit=0)\\n            \\n            # Iterate through top-level comments\\n            top_comments = [comment for comment in post.comments if isinstance(comment, praw.models.Comment)]\\n            \\n            # Limit the number of comments to comment_limit\\n            for comment in top_comments[:comment_limit]:  \\n                comments_data.append({\\n                    \\'subreddit\\': subreddit_name,\\n                    \\'comment\\': comment.body,\\n                    \\'comment_author\\': comment.author.name if comment.author else \\'[deleted]\\',\\n                    \\'author_karma\\': (comment.author.link_karma + comment.author.comment_karma) if comment.author else 0,\\n                    \\'post_title\\': post.title,\\n                    \\'post_content\\': post.selftext,\\n                    \\'post_upvotes\\': post.score,\\n                    \\'timestamp\\': datetime.fromtimestamp(comment.created_utc).strftime(\\'%Y-%m-%d %H:%M:%S\\'),\\n                })\\n            \\n            print(f\"Collected {len(top_comments[:comment_limit])} comments from post {post_id}\")\\n        \\n        except Exception as e:\\n            print(f\"Error collecting comments from post {post_id}: {e}\")\\n        \\n        # Optional: Small delay to avoid hitting rate limits\\n        time.sleep(1) \\n    \\n    return pd.DataFrame(comments_data)\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def collect_top_level_comments_from_csv(csv_file, comment_limit=2):\n",
    "    # Read posts from CSV file\n",
    "    posts_df = pd.read_csv(csv_file)\n",
    "    comments_data = []\n",
    "\n",
    "    # Iterate through each post in the CSV file\n",
    "    for _, row in posts_df.iterrows():\n",
    "        post_id = row['post_id']  # 'post_id' column from your CSV\n",
    "        subreddit_name = row['subreddit']  # 'subreddit' column from your CSV\n",
    "        \n",
    "        try:\n",
    "            # Fetch the post using the post ID\n",
    "            post = reddit.submission(id=post_id)\n",
    "            post.comments.replace_more(limit=0)  # Ensure all top-level comments are loaded\n",
    "            \n",
    "            # Debug: Print post details to confirm it's being processed\n",
    "            print(f\"Processing post {post_id} in subreddit {subreddit_name}\")\n",
    "            \n",
    "            # Check if the post has comments\n",
    "            if not post.comments:\n",
    "                print(f\"No comments found for post {post_id}\")\n",
    "                continue\n",
    "\n",
    "            # Collect top-level comments\n",
    "            for comment in post.comments[:comment_limit]:  # Limit to the top `comment_limit` comments\n",
    "                if comment.parent_id.split('_')[1] == post_id:  # Ensure it's a top-level comment\n",
    "                    \n",
    "                    # Get comment author and karma (handle potential missing authors)\n",
    "                    author_karma = 0\n",
    "                    author_name = \"[deleted]\"\n",
    "                    \n",
    "                    if comment.author:\n",
    "                        author_name = comment.author.name\n",
    "                        author_karma = comment.author.link_karma + comment.author.comment_karma  # Total karma\n",
    "\n",
    "                    comments_data.append({\n",
    "                        'subreddit': subreddit_name,\n",
    "                        'comment': comment.body,\n",
    "                        'comment_author': author_name,\n",
    "                        'author_karma': author_karma,\n",
    "                        'post_title': post.title,\n",
    "                        'post_content': post.selftext,\n",
    "                        'post_upvotes': post.score,\n",
    "                        'timestamp': datetime.fromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    })\n",
    "            \n",
    "            print(f\"Collected {len(comments_data)} comments from post {post_id}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error collecting comments from post {post_id}: {e}\")\n",
    "        \n",
    "        # Optional: Small delay to avoid hitting rate limits\n",
    "        time.sleep(1) \n",
    "    \n",
    "    # Return the DataFrame of collected comments\n",
    "    return pd.DataFrame(comments_data)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def collect_top_level_comments(csv_file, comment_limit=2):\n",
    "    # Read the CSV containing post information\n",
    "    posts_df = pd.read_csv(csv_file)\n",
    "    comments_data = []\n",
    "\n",
    "    # Loop through the DataFrame rows and extract post_id and subreddit\n",
    "    for _, row in posts_df.iterrows():\n",
    "        post_id = row['post_id']  # Ensure the CSV has a 'post_id' column\n",
    "        subreddit_name = row['subreddit']  # Ensure the CSV has a 'subreddit' column\n",
    "\n",
    "        try:\n",
    "            # Get the post by its ID\n",
    "            post = reddit.submission(id=post_id)\n",
    "            post.comments.replace_more(limit=0)  # Ensure we get all comments\n",
    "\n",
    "            # Debugging: Print post details to ensure it's fetched\n",
    "            print(f\"Processing post {post_id} from subreddit {subreddit_name}\")\n",
    "\n",
    "            # Collect top-level comments (limit to 'comment_limit' comments per post)\n",
    "            for comment in post.comments[:comment_limit]:  # Limiting the number of comments\n",
    "                if comment.parent_id.split('_')[1] == post_id:  # Check if it's a top-level comment\n",
    "                    # Initialize author details\n",
    "                    author_karma = 0\n",
    "                    author_name = \"[deleted]\"\n",
    "                    \n",
    "                    if comment.author:\n",
    "                        author_name = comment.author.name\n",
    "                        author_karma = comment.author.link_karma + comment.author.comment_karma  # Total karma\n",
    "\n",
    "                    # Add the comment data to the list\n",
    "                    comments_data.append({\n",
    "                        'subreddit': subreddit_name,\n",
    "                        'comment': comment.body,\n",
    "                        'comment_author': author_name,\n",
    "                        'author_karma': author_karma,\n",
    "                        'post_title': post.title,\n",
    "                        'post_content': post.selftext,\n",
    "                        'post_upvotes': post.score,\n",
    "                        'timestamp': datetime.fromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    })\n",
    "\n",
    "            print(f\"Collected {len(comments_data)} comments from post {post_id}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error collecting comments from post {post_id}: {e}\")\n",
    "        \n",
    "        # Small delay to avoid Reddit API rate limits\n",
    "        time.sleep(1) \n",
    "    \n",
    "    # Return the collected comments as a DataFrame\n",
    "    return pd.DataFrame(comments_data)\n",
    "\n",
    "def save_comments_to_csv(csv_file, comment_limit=2):\n",
    "    comments_df = collect_top_level_comments(csv_file, comment_limit)\n",
    "    \n",
    "    # Check if the DataFrame has data\n",
    "    if comments_df.empty:\n",
    "        print(\"No comments collected.\")\n",
    "    else:\n",
    "        comments_df.to_csv(\"top_level_comments.csv\", index=False)\n",
    "        print(\"\\nComments saved to collected_comments_with_author_and_karma.csv\")\n",
    "\n",
    "\"\"\"\n",
    "    # Read the post IDs and other data from the provided CSV file\n",
    "    posts_df = pd.read_csv(csv_file)\n",
    "    comments_data = []\n",
    "    \n",
    "    for _, row in posts_df.iterrows():\n",
    "        post_id = row['post_id']\n",
    "        subreddit_name = row['subreddit']\n",
    "        \n",
    "        try:\n",
    "            # Fetch the post using the post_id\n",
    "            post = reddit.submission(id=post_id)\n",
    "            \n",
    "            # Replace \"MoreComments\" with empty (expand all top-level comments)\n",
    "            post.comments.replace_more(limit=0)\n",
    "            \n",
    "            # Iterate through top-level comments\n",
    "            top_comments = [comment for comment in post.comments if isinstance(comment, praw.models.Comment)]\n",
    "            \n",
    "            # Limit the number of comments to comment_limit\n",
    "            for comment in top_comments[:comment_limit]:  \n",
    "                comments_data.append({\n",
    "                    'subreddit': subreddit_name,\n",
    "                    'comment': comment.body,\n",
    "                    'comment_author': comment.author.name if comment.author else '[deleted]',\n",
    "                    'author_karma': (comment.author.link_karma + comment.author.comment_karma) if comment.author else 0,\n",
    "                    'post_title': post.title,\n",
    "                    'post_content': post.selftext,\n",
    "                    'post_upvotes': post.score,\n",
    "                    'timestamp': datetime.fromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                })\n",
    "            \n",
    "            print(f\"Collected {len(top_comments[:comment_limit])} comments from post {post_id}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error collecting comments from post {post_id}: {e}\")\n",
    "        \n",
    "        # Optional: Small delay to avoid hitting rate limits\n",
    "        time.sleep(1) \n",
    "    \n",
    "    return pd.DataFrame(comments_data)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Example usage\n",
    "# save_comments_to_csv(\"reddit_posts.csv\", comment_limit=2)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Analyze and Label Engagement for Posts\n",
    "Calculates engagement scores and labels posts with engagement levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_engagement(post):\n",
    "    upvotes = post['upvotes']\n",
    "    comments_count = post['comments_count']\n",
    "    subscribers = post['subscriber_count']\n",
    "    \n",
    "    if subscribers > 0:\n",
    "        engagement = (upvotes + comments_count) / subscribers\n",
    "    else:\n",
    "        engagement = 0  \n",
    "    return engagement\n",
    "\n",
    "def define_engagement(posts_df):\n",
    "    def label_engagement(score):\n",
    "        if score < 0.0025:\n",
    "            return \"Low\"\n",
    "        elif score < 0.0050:\n",
    "            return \"Medium\"\n",
    "        else:\n",
    "            return \"High\"\n",
    "    posts_df['engagement_level'] = posts_df['normalized_engagement'].apply(label_engagement)\n",
    "    return posts_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Define Engagement Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_engagement(posts_df):\n",
    "    def label_engagement(score):\n",
    "        if score < 1:\n",
    "            return \"Low\"\n",
    "        elif score < 5:\n",
    "            return \"Medium\"\n",
    "        else:\n",
    "            return \"High\"\n",
    "    \n",
    "    posts_df['engagement_level'] = posts_df['normalized_engagement'].apply(label_engagement)\n",
    "    return posts_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Add Features to Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features_to_posts(df):\n",
    "    df['title_length'] = df['title'].apply(len)\n",
    "    df['post_length'] = df['content'].apply(lambda x: len(str(x)) if pd.notnull(x) else 0)\n",
    "    \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "    \n",
    "    df['time_of_day'] = df['timestamp'].dt.hour\n",
    "    df['day_of_week'] = df['timestamp'].dt.day_name()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Running All Methods\n",
    "Finally, running all the methods in order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting posts from r/WritingPrompts\n",
      "Collected 4 posts from r/WritingPrompts\n",
      "Collecting posts from r/TrueOffMyChest\n",
      "Collected 4 posts from r/TrueOffMyChest\n",
      "Collecting posts from r/NoSleep\n",
      "Collected 4 posts from r/NoSleep\n",
      "Collecting posts from r/ExplainLikeImFive\n",
      "Collected 4 posts from r/ExplainLikeImFive\n",
      "Collecting posts from r/IAmA\n",
      "Collected 4 posts from r/IAmA\n",
      "Collecting posts from r/CasualConversation\n",
      "Collected 4 posts from r/CasualConversation\n",
      "Collecting posts from r/TrueAskReddit\n",
      "Collected 4 posts from r/TrueAskReddit\n",
      "Collecting posts from r/Confession\n",
      "Collected 4 posts from r/Confession\n",
      "Collecting posts from r/relationships\n",
      "Collected 4 posts from r/relationships\n",
      "Collecting posts from r/ShortScaryStories\n",
      "Collected 4 posts from r/ShortScaryStories\n",
      "Collecting posts from r/ProRevenge\n",
      "Collected 4 posts from r/ProRevenge\n",
      "Collecting posts from r/NuclearRevenge\n",
      "Collected 4 posts from r/NuclearRevenge\n",
      "Collecting posts from r/LifeProTips\n",
      "Collected 4 posts from r/LifeProTips\n",
      "Collecting posts from r/needadvice\n",
      "Collected 4 posts from r/needadvice\n",
      "Collecting posts from r/TrueUnpopularOpinion\n",
      "Collected 4 posts from r/TrueUnpopularOpinion\n",
      "\n",
      "All posts data saved to subreddit_posts.csv\n",
      "\n",
      "Summary of collected posts:\n",
      "subreddit\n",
      "WritingPrompts          4\n",
      "TrueOffMyChest          4\n",
      "NoSleep                 4\n",
      "ExplainLikeImFive       4\n",
      "IAmA                    4\n",
      "CasualConversation      4\n",
      "TrueAskReddit           4\n",
      "Confession              4\n",
      "relationships           4\n",
      "ShortScaryStories       4\n",
      "ProRevenge              4\n",
      "NuclearRevenge          4\n",
      "LifeProTips             4\n",
      "needadvice              4\n",
      "TrueUnpopularOpinion    4\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Local\\Temp\\ipykernel_2136\\3403129084.py:11: DeprecationWarning: Calling SubredditRules to get a list of rules is deprecated. Remove the parentheses to use the iterator. View the PRAW documentation on how to change the code in order to use the iterator (https://praw.readthedocs.io/en/latest/code_overview/other/subredditrules.html#praw.models.reddit.rules.SubredditRules.__call__).\n",
      "  rules = list(subreddit.rules())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not fetch rules for r/WritingPrompts: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/WritingPrompts: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/TrueOffMyChest: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/TrueOffMyChest: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/NoSleep: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/NoSleep: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/ExplainLikeImFive: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/ExplainLikeImFive: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/IAmA: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/IAmA: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/CasualConversation: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/CasualConversation: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/TrueAskReddit: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/TrueAskReddit: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/Confession: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/Confession: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/relationships: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/relationships: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/ShortScaryStories: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/ShortScaryStories: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/ProRevenge: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/ProRevenge: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/NuclearRevenge: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/NuclearRevenge: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/LifeProTips: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/LifeProTips: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/needadvice: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/needadvice: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/TrueUnpopularOpinion: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/TrueUnpopularOpinion: 'Subreddit' object has no attribute 'moderators'\n",
      "\n",
      "Subreddit-level data saved to subreddit_level_data.csv\n",
      "Processing post 1gnf39g from subreddit WritingPrompts\n",
      "Collected 2 comments from post 1gnf39g\n",
      "Processing post 1gp869q from subreddit WritingPrompts\n",
      "Collected 4 comments from post 1gp869q\n",
      "Processing post 1gqqvjy from subreddit WritingPrompts\n",
      "Collected 6 comments from post 1gqqvjy\n",
      "Processing post 1gqx1eg from subreddit WritingPrompts\n",
      "Collected 8 comments from post 1gqx1eg\n",
      "Processing post 18rtwhw from subreddit TrueOffMyChest\n",
      "Collected 10 comments from post 18rtwhw\n",
      "Processing post 1gl2ugq from subreddit TrueOffMyChest\n",
      "Collected 11 comments from post 1gl2ugq\n",
      "Processing post 1gqu4o6 from subreddit TrueOffMyChest\n",
      "Collected 13 comments from post 1gqu4o6\n",
      "Processing post 1gqnw79 from subreddit TrueOffMyChest\n",
      "Collected 15 comments from post 1gqnw79\n",
      "Processing post 1gem7uq from subreddit NoSleep\n",
      "Collected 15 comments from post 1gem7uq\n",
      "Processing post 1gquelk from subreddit NoSleep\n",
      "Collected 17 comments from post 1gquelk\n",
      "Processing post 1gqen62 from subreddit NoSleep\n",
      "Collected 19 comments from post 1gqen62\n",
      "Processing post 1gqucq4 from subreddit NoSleep\n",
      "Collected 21 comments from post 1gqucq4\n",
      "Processing post 1gkjas4 from subreddit ExplainLikeImFive\n",
      "Collected 23 comments from post 1gkjas4\n",
      "Processing post 1gqdo0i from subreddit ExplainLikeImFive\n",
      "Collected 25 comments from post 1gqdo0i\n",
      "Processing post 1gqvp2h from subreddit ExplainLikeImFive\n",
      "Collected 27 comments from post 1gqvp2h\n",
      "Processing post 1gqw0z7 from subreddit ExplainLikeImFive\n",
      "Collected 28 comments from post 1gqw0z7\n",
      "Processing post 1gpwowe from subreddit IAmA\n",
      "Collected 30 comments from post 1gpwowe\n",
      "Processing post 1gr1b2h from subreddit IAmA\n",
      "Collected 32 comments from post 1gr1b2h\n",
      "Processing post 1gqal47 from subreddit IAmA\n",
      "Collected 34 comments from post 1gqal47\n",
      "Processing post 1gqhkvy from subreddit IAmA\n",
      "Collected 36 comments from post 1gqhkvy\n",
      "Processing post 1gh7esa from subreddit CasualConversation\n",
      "Collected 38 comments from post 1gh7esa\n",
      "Processing post 1gqvcff from subreddit CasualConversation\n",
      "Collected 40 comments from post 1gqvcff\n",
      "Processing post 1gqvvqq from subreddit CasualConversation\n",
      "Collected 42 comments from post 1gqvvqq\n",
      "Processing post 1gqm7hd from subreddit CasualConversation\n",
      "Collected 44 comments from post 1gqm7hd\n",
      "Processing post 1gqcrb2 from subreddit TrueAskReddit\n",
      "Collected 46 comments from post 1gqcrb2\n",
      "Processing post 1gqlymw from subreddit TrueAskReddit\n",
      "Collected 48 comments from post 1gqlymw\n",
      "Processing post 1gphr3y from subreddit TrueAskReddit\n",
      "Collected 50 comments from post 1gphr3y\n",
      "Processing post 1gq9gv8 from subreddit TrueAskReddit\n",
      "Collected 52 comments from post 1gq9gv8\n",
      "Processing post 126xkib from subreddit Confession\n",
      "Collected 52 comments from post 126xkib\n",
      "Processing post 1gqy8qj from subreddit Confession\n",
      "Collected 54 comments from post 1gqy8qj\n",
      "Processing post 1gqg3v3 from subreddit Confession\n",
      "Collected 56 comments from post 1gqg3v3\n",
      "Processing post 1gqfkfl from subreddit Confession\n",
      "Collected 58 comments from post 1gqfkfl\n",
      "Processing post 1ge6159 from subreddit relationships\n",
      "Collected 58 comments from post 1ge6159\n",
      "Processing post 1gqcupt from subreddit relationships\n",
      "Collected 60 comments from post 1gqcupt\n",
      "Processing post 1gqzi5n from subreddit relationships\n",
      "Collected 62 comments from post 1gqzi5n\n",
      "Processing post 1gr15r1 from subreddit relationships\n",
      "Collected 64 comments from post 1gr15r1\n",
      "Processing post q6cw0j from subreddit ShortScaryStories\n",
      "Collected 66 comments from post q6cw0j\n",
      "Processing post 1gqqy1b from subreddit ShortScaryStories\n",
      "Collected 68 comments from post 1gqqy1b\n",
      "Processing post 1gqfbmz from subreddit ShortScaryStories\n",
      "Collected 70 comments from post 1gqfbmz\n",
      "Processing post 1gqhh8a from subreddit ShortScaryStories\n",
      "Collected 72 comments from post 1gqhh8a\n",
      "Processing post 1gm2riq from subreddit ProRevenge\n",
      "Collected 74 comments from post 1gm2riq\n",
      "Processing post 1g13tpw from subreddit ProRevenge\n",
      "Collected 76 comments from post 1g13tpw\n",
      "Processing post 1fova4p from subreddit ProRevenge\n",
      "Collected 78 comments from post 1fova4p\n",
      "Processing post 1exnr27 from subreddit ProRevenge\n",
      "Collected 80 comments from post 1exnr27\n",
      "Processing post unlvd0 from subreddit NuclearRevenge\n",
      "Collected 81 comments from post unlvd0\n",
      "Processing post 163v9oo from subreddit NuclearRevenge\n",
      "Collected 83 comments from post 163v9oo\n",
      "Processing post 1gqz9j4 from subreddit NuclearRevenge\n",
      "Collected 85 comments from post 1gqz9j4\n",
      "Processing post 1ftajvn from subreddit NuclearRevenge\n",
      "Collected 87 comments from post 1ftajvn\n",
      "Processing post 1gmsp7q from subreddit LifeProTips\n",
      "Collected 89 comments from post 1gmsp7q\n",
      "Processing post 1gqwgv6 from subreddit LifeProTips\n",
      "Collected 91 comments from post 1gqwgv6\n",
      "Processing post 1gqjl2k from subreddit LifeProTips\n",
      "Collected 93 comments from post 1gqjl2k\n",
      "Processing post 1gqqivs from subreddit LifeProTips\n",
      "Collected 95 comments from post 1gqqivs\n",
      "Processing post 1aqgnxa from subreddit needadvice\n",
      "Collected 97 comments from post 1aqgnxa\n",
      "Processing post 1gqyzy2 from subreddit needadvice\n",
      "Collected 98 comments from post 1gqyzy2\n",
      "Processing post 1gqqaym from subreddit needadvice\n",
      "Collected 100 comments from post 1gqqaym\n",
      "Processing post 1gq9wxk from subreddit needadvice\n",
      "Collected 102 comments from post 1gq9wxk\n",
      "Processing post 1g9ojlt from subreddit TrueUnpopularOpinion\n",
      "Collected 104 comments from post 1g9ojlt\n",
      "Processing post ncm4ou from subreddit TrueUnpopularOpinion\n",
      "Collected 106 comments from post ncm4ou\n",
      "Processing post 1gqqu5p from subreddit TrueUnpopularOpinion\n",
      "Collected 108 comments from post 1gqqu5p\n",
      "Processing post 1gqqz07 from subreddit TrueUnpopularOpinion\n",
      "Collected 110 comments from post 1gqqz07\n",
      "\n",
      "Comments saved to collected_comments_with_author_and_karma.csv\n"
     ]
    }
   ],
   "source": [
    "# Collect posts from all subreddits\n",
    "all_posts_df = collect_posts_from_subreddits(subreddit_list, post_limit=4)\n",
    "\n",
    "# Save all collected posts to a single CSV file\n",
    "csv_filename = \"subreddit_posts.csv\"\n",
    "all_posts_df.to_csv(csv_filename, index=False)\n",
    "print(f\"\\nAll posts data saved to {csv_filename}\")\n",
    "\n",
    "# Print summary of collected posts\n",
    "print(\"\\nSummary of collected posts:\")\n",
    "print(all_posts_df['subreddit'].value_counts())\n",
    "\n",
    "# Collect subreddit-level data (e.g., rules, moderation, etc.)\n",
    "subreddit_level_data = collect_subreddit_level_data(reddit, subreddit_list, limit=20)\n",
    "\n",
    "# Convert to DataFrame and save to CSV\n",
    "subreddit_level_df = pd.DataFrame(subreddit_level_data)\n",
    "subreddit_level_df.to_csv(\"subreddit_level_data.csv\", index=False)\n",
    "print(\"\\nSubreddit-level data saved to subreddit_level_data.csv\")\n",
    "\n",
    "\n",
   "cleaned_labeled_data = pd.read_csv("cleaned_labeled_subreddit_posts.csv")\n",
   "cleaned_labeled_data = cleaned_labeled_data.merge(subreddit_level_df[['subreddit_name', 'subscriber_count']], 
                                                  left_on='subreddit', right_on='subreddit_name', how='left')\n",

"cleaned_labeled_data['engagement'] = cleaned_labeled_data.apply(calculate_engagement, axis=1)\n",
"cleaned_labeled_data['normalized_engagement'] = cleaned_labeled_data['engagement'] * 10000\n",

"cleaned_labeled_data = define_engagement(cleaned_labeled_data)\n",

"cleaned_labeled_data.to_csv("labeled_subreddit_posts.csv", index=False)\n",
"print("\nUpdated data with engagement levels saved to labeled_subreddit_posts.csv")\n",

"print("\nDisplaying updated dataframe with engagement scores:")\n",
"print(cleaned_labeled_data.head()) \n" 
   
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL CLEANED CSV Creation and Storing\n",
    "In this Code Block we will be creating a csv file and storing all our scraped data."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
