{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAH770GMcFcy"
   },
   "source": [
    "## Project Overview\n",
    "\n",
    "### Problem Statement\n",
    "In today's digital world, understanding how Reddit communities function is crucial for moderators, users, and researchers...\n",
    "\n",
    "### Data Collection Overview\n",
    "- **Tools**:\n",
    "  - PRAW (Python Reddit API Wrapper)\n",
    "  - BeautifulSoup / Scrapy\n",
    "  \n",
    "- **Data Points to Collect**:\n",
    "  - **Posts**: Title, content, upvotes...\n",
    "  - **Comments**: Content, upvotes...\n",
    "  \n",
    "### Solution Approach\n",
    "1. **Sentiment & Engagement Analysis**\n",
    "   - Visualizations using Matplotlib and Seaborn...\n",
    "   \n",
    "2. **Correlation Analysis**\n",
    "   - Apply classification algorithms using scikit-learn...\n",
    "\n",
    "### Expected Deliverables\n",
    "- Insight Report\n",
    "- Actionable Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1xAJ3tSeEx6"
   },
   "source": [
    "## Setup Environment\n",
    "\n",
    "### Purpose\n",
    "This section prepares our Google Colab environment for the Reddit Communities analysis project as outlined in our team's proposal. We'll install the necessary Python libraries to handle data collection, processing, analysis, and visualization.\n",
    "\n",
    "### Key Libraries\n",
    "- PRAW: For accessing the Reddit API\n",
    "- pandas: For data manipulation and analysis\n",
    "- numpy: For numerical computing\n",
    "- matplotlib and seaborn: For data visualization\n",
    "- nltk: For natural language processing and sentiment analysis\n",
    "- scikit-learn: For machine learning tasks\n",
    "\n",
    "### Alignment with Project Goals\n",
    "These libraries support our objectives of:\n",
    "1. Analyzing moderation strategies\n",
    "2. Predicting post impact\n",
    "3. Visualizing Reddit community interactions\n",
    "\n",
    "### Installation Code\n",
    "Run the following cell to install the required libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MCitsxPEa-Kb",
    "outputId": "5c380300-ea61-4a94-e37a-c18e3f3d1fe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in d:\\uicsem1\\introtodatascience\\class-project-cheesy-little-explorers\\venv\\lib\\site-packages (7.8.1)\n",
      "Requirement already satisfied: prawcore<3,>=2.4 in d:\\uicsem1\\introtodatascience\\class-project-cheesy-little-explorers\\venv\\lib\\site-packages (from praw) (2.4.0)\n",
      "Requirement already satisfied: update_checker>=0.18 in d:\\uicsem1\\introtodatascience\\class-project-cheesy-little-explorers\\venv\\lib\\site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in d:\\uicsem1\\introtodatascience\\class-project-cheesy-little-explorers\\venv\\lib\\site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in d:\\uicsem1\\introtodatascience\\class-project-cheesy-little-explorers\\venv\\lib\\site-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\uicsem1\\introtodatascience\\class-project-cheesy-little-explorers\\venv\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\uicsem1\\introtodatascience\\class-project-cheesy-little-explorers\\venv\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\uicsem1\\introtodatascience\\class-project-cheesy-little-explorers\\venv\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\uicsem1\\introtodatascience\\class-project-cheesy-little-explorers\\venv\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import seaborn\n",
    "import nltk\n",
    "%pip install praw\n",
    "import praw\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Praw API Config\n",
    "\n",
    "This block of code create tje connection with the reddit application and by authenticating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "science\n",
      "Reddit Science\n"
     ]
    }
   ],
   "source": [
    "def setup_reddit_api():\n",
    "    return praw.Reddit(\n",
    "        client_id=\"nrakGjG_wnBE_5UdcHNJoQ\",\n",
    "        client_secret=\"qmGr1q_4pGIBR0pYJE8cyhUbTbdX2w\",\n",
    "        user_agent=\"LittleCheesyExplorers/1.0 (Reddit Communities Analysis Project)\"\n",
    "    )\n",
    "\n",
    "reddit = setup_reddit_api()\n",
    "\n",
    "print(reddit.user.me())\n",
    "subreddit = reddit.subreddit(\"science\")\n",
    "print(subreddit.display_name)\n",
    "print(subreddit.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read required Subreddits \n",
    "This code block will store the names of the subreddits that we want to collect data from.\n",
    "\n",
    "The names will be stored in a text file and we will read from that and scrape based on that list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AskReddit', 'news', 'funny', 'gaming', 'todayilearned', 'science']\n"
     ]
    }
   ],
   "source": [
    "# Read subreddit names from the text file\n",
    "with open('subreddits.txt', 'r') as file:\n",
    "    subreddit_list = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "print(subreddit_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collection Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_subreddit_posts(subreddit_name, post_limit=10):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts_data = []\n",
    "\n",
    "    for post in subreddit.hot(limit=post_limit):\n",
    "        posts_data.append({\n",
    "            'subreddit': subreddit_name,\n",
    "            'title': post.title,\n",
    "            'content': post.selftext,\n",
    "            'upvotes': post.score,\n",
    "            'upvote_ratio': post.upvote_ratio,\n",
    "            'comments_count': post.num_comments,\n",
    "            'author': post.author.name if post.author else '[deleted]',\n",
    "            'timestamp': datetime.fromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'post_id': post.id\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(posts_data)\n",
    "\n",
    "def collect_posts_from_subreddits(subreddit_list, post_limit=10):\n",
    "    all_posts = []\n",
    "    \n",
    "    for subreddit_name in subreddit_list:\n",
    "        print(f\"Collecting posts from r/{subreddit_name}\")\n",
    "        try:\n",
    "            posts_df = collect_subreddit_posts(subreddit_name, post_limit)\n",
    "            all_posts.append(posts_df)\n",
    "            print(f\"Collected {len(posts_df)} posts from r/{subreddit_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error collecting posts from r/{subreddit_name}: {str(e)}\")\n",
    "    \n",
    "    combined_df = pd.concat(all_posts, ignore_index=True)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Collection Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting posts from r/AskReddit\n",
      "Collected 851 posts from r/AskReddit\n",
      "Collecting posts from r/news\n",
      "Collected 203 posts from r/news\n",
      "Collecting posts from r/funny\n",
      "Collected 360 posts from r/funny\n",
      "Collecting posts from r/gaming\n",
      "Collected 150 posts from r/gaming\n",
      "Collecting posts from r/todayilearned\n",
      "Collected 504 posts from r/todayilearned\n",
      "Collecting posts from r/science\n",
      "Collected 774 posts from r/science\n",
      "\n",
      "All posts data saved to subreddit_posts.csv\n",
      "\n",
      "Summary of collected posts:\n",
      "subreddit\n",
      "AskReddit        851\n",
      "science          774\n",
      "todayilearned    504\n",
      "funny            360\n",
      "news             203\n",
      "gaming           150\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample post data:\n",
      "subreddit                                                 AskReddit\n",
      "title                           2024 United States Elections Thread\n",
      "content           Please use this thread to discuss the ongoing ...\n",
      "upvotes                                                          98\n",
      "upvote_ratio                                                   0.68\n",
      "comments_count                                                 6807\n",
      "author                                          AskRedditModerators\n",
      "timestamp                                       2024-11-05 17:14:10\n",
      "post_id                                                     1gkk9s3\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Collect posts from all subreddits\n",
    "all_posts_df = collect_posts_from_subreddits(subreddit_list, post_limit=10000)\n",
    "\n",
    "# Save all collected posts to a single CSV file\n",
    "csv_filename = \"subreddit_posts.csv\"\n",
    "all_posts_df.to_csv(csv_filename, index=False)\n",
    "print(f\"\\nAll posts data saved to {csv_filename}\")\n",
    "\n",
    "# Print summary of collected posts\n",
    "print(\"\\nSummary of collected posts:\")\n",
    "print(all_posts_df['subreddit'].value_counts())\n",
    "\n",
    "# Sample data output\n",
    "print(\"\\nSample post data:\")\n",
    "print(all_posts_df.iloc[0] if len(all_posts_df) > 0 else \"No posts collected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV Creation and Storing\n",
    "In this Code Block we will be creating a csv file and storing all our scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
