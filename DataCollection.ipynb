{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAH770GMcFcy"
   },
   "source": [
    "## Project Overview\n",
    "\n",
    "### Problem Statement\n",
    "In today's digital world, understanding how Reddit communities function is crucial for moderators, users, and researchers...\n",
    "\n",
    "### Data Collection Overview\n",
    "- **Tools**:\n",
    "  - PRAW (Python Reddit API Wrapper)\n",
    "  - BeautifulSoup / Scrapy\n",
    "  \n",
    "- **Data Points to Collect**:\n",
    "  - **Posts**: Title, content, upvotes...\n",
    "  - **Comments**: Content, upvotes...\n",
    "  \n",
    "### Solution Approach\n",
    "1. **Sentiment & Engagement Analysis**\n",
    "   - Visualizations using Matplotlib and Seaborn...\n",
    "   \n",
    "2. **Correlation Analysis**\n",
    "   - Apply classification algorithms using scikit-learn...\n",
    "\n",
    "### Expected Deliverables\n",
    "- Insight Report\n",
    "- Actionable Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1xAJ3tSeEx6"
   },
   "source": [
    "## Setup Environment\n",
    "\n",
    "### Purpose\n",
    "This section prepares our Google Colab environment for the Reddit Communities analysis project as outlined in our team's proposal. We'll install the necessary Python libraries to handle data collection, processing, analysis, and visualization.\n",
    "\n",
    "### Key Libraries\n",
    "- PRAW: For accessing the Reddit API\n",
    "- pandas: For data manipulation and analysis\n",
    "- numpy: For numerical computing\n",
    "- matplotlib and seaborn: For data visualization\n",
    "- nltk: For natural language processing and sentiment analysis\n",
    "- scikit-learn: For machine learning tasks\n",
    "\n",
    "### Alignment with Project Goals\n",
    "These libraries support our objectives of:\n",
    "1. Analyzing moderation strategies\n",
    "2. Predicting post impact\n",
    "3. Visualizing Reddit community interactions\n",
    "\n",
    "### Installation Code\n",
    "Run the following cell to install the required libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MCitsxPEa-Kb",
    "outputId": "5c380300-ea61-4a94-e37a-c18e3f3d1fe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in d:\\uicsem1\\introtodatascience\\class-project-cheesy-little-explorers\\venv\\lib\\site-packages (7.8.1)\n",
      "Requirement already satisfied: prawcore<3,>=2.4 in d:\\uicsem1\\introtodatascience\\class-project-cheesy-little-explorers\\venv\\lib\\site-packages (from praw) (2.4.0)\n",
      "Requirement already satisfied: update_checker>=0.18 in d:\\uicsem1\\introtodatascience\\class-project-cheesy-little-explorers\\venv\\lib\\site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in d:\\uicsem1\\introtodatascience\\class-project-cheesy-little-explorers\\venv\\lib\\site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in d:\\uicsem1\\introtodatascience\\class-project-cheesy-little-explorers\\venv\\lib\\site-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\uicsem1\\introtodatascience\\class-project-cheesy-little-explorers\\venv\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\uicsem1\\introtodatascience\\class-project-cheesy-little-explorers\\venv\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\uicsem1\\introtodatascience\\class-project-cheesy-little-explorers\\venv\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\uicsem1\\introtodatascience\\class-project-cheesy-little-explorers\\venv\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import seaborn\n",
    "import nltk\n",
    "%pip install praw\n",
    "import praw\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Praw API Config\n",
    "\n",
    "This block of code create tje connection with the reddit application and by authenticating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "science\n",
      "Reddit Science\n"
     ]
    }
   ],
   "source": [
    "def setup_reddit_api():\n",
    "    return praw.Reddit(\n",
    "        client_id=\"nrakGjG_wnBE_5UdcHNJoQ\",\n",
    "        client_secret=\"qmGr1q_4pGIBR0pYJE8cyhUbTbdX2w\",\n",
    "        user_agent=\"LittleCheesyExplorers/1.0 (Reddit Communities Analysis Project)\"\n",
    "    )\n",
    "\n",
    "reddit = setup_reddit_api()\n",
    "\n",
    "print(reddit.user.me())\n",
    "subreddit = reddit.subreddit(\"science\")\n",
    "print(subreddit.display_name)\n",
    "print(subreddit.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read required Subreddits \n",
    "This code block will store the names of the subreddits that we want to collect data from.\n",
    "\n",
    "The names will be stored in a text file and we will read from that and scrape based on that list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AskReddit', 'news', 'funny', 'gaming', 'todayilearned', 'science']\n"
     ]
    }
   ],
   "source": [
    "# Read subreddit names from the text file\n",
    "with open('subreddits.txt', 'r') as file:\n",
    "    subreddit_list = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "print(subreddit_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collection Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_subreddit_posts(subreddit_name, post_limit=10):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts_data = []\n",
    "\n",
    "    for post in subreddit.hot(limit=post_limit):\n",
    "        posts_data.append({\n",
    "            'subreddit': subreddit_name,\n",
    "            'title': post.title,\n",
    "            'content': post.selftext,\n",
    "            'upvotes': post.score,\n",
    "            'upvote_ratio': post.upvote_ratio,\n",
    "            'comments_count': post.num_comments,\n",
    "            'author': post.author.name if post.author else '[deleted]',\n",
    "            'timestamp': datetime.fromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'post_id': post.id\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(posts_data)\n",
    "\n",
    "def collect_posts_from_subreddits(subreddit_list, post_limit=10):\n",
    "    all_posts = []\n",
    "    \n",
    "    for subreddit_name in subreddit_list:\n",
    "        print(f\"Collecting posts from r/{subreddit_name}\")\n",
    "        try:\n",
    "            posts_df = collect_subreddit_posts(subreddit_name, post_limit)\n",
    "            all_posts.append(posts_df)\n",
    "            print(f\"Collected {len(posts_df)} posts from r/{subreddit_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error collecting posts from r/{subreddit_name}: {str(e)}\")\n",
    "    \n",
    "    combined_df = pd.concat(all_posts, ignore_index=True)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Collection Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'collect_posts_from_subreddits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Collect posts from all subreddits\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m all_posts_df \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_posts_from_subreddits\u001b[49m(subreddit_list, post_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Save all collected posts to a single CSV file\u001b[39;00m\n\u001b[0;32m      5\u001b[0m csv_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubreddit_posts.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'collect_posts_from_subreddits' is not defined"
     ]
    }
   ],
   "source": [
    "# Collect posts from all subreddits\n",
    "all_posts_df = collect_posts_from_subreddits(subreddit_list, post_limit=10000)\n",
    "\n",
    "# Save all collected posts to a single CSV file\n",
    "csv_filename = \"subreddit_posts.csv\"\n",
    "all_posts_df.to_csv(csv_filename, index=False)\n",
    "print(f\"\\nAll posts data saved to {csv_filename}\")\n",
    "\n",
    "# Print summary of collected posts\n",
    "print(\"\\nSummary of collected posts:\")\n",
    "print(all_posts_df['subreddit'].value_counts())\n",
    "\n",
    "# Sample data output\n",
    "print(\"\\nSample post data:\")\n",
    "print(all_posts_df.iloc[0] if len(all_posts_df) > 0 else \"No posts collected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New features added and file saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# adding derived columns \n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the existing CSV file\n",
    "df = pd.read_csv('subreddit_posts.csv')\n",
    "\n",
    "# Calculate Title Length\n",
    "df['title_length'] = df['title'].apply(len)\n",
    "\n",
    "# Calculate Post Length (use content column, assuming it's a text-based post)\n",
    "df['post_length'] = df['content'].apply(lambda x: len(str(x)) if pd.notnull(x) else 0)\n",
    "\n",
    "# Convert the timestamp to  datetime format\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "\n",
    "# Calculate Time of Day (hour of the day)\n",
    "df['time_of_day'] = df['timestamp'].dt.hour\n",
    "\n",
    "# Calculate Day of the Week\n",
    "df['day_of_week'] = df['timestamp'].dt.day_name()\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv('subreddit_posts.csv', index=False)\n",
    "\n",
    "print(\"New features added and file saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV Creation and Storing\n",
    "In this Code Block we will be creating a csv file and storing all our scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
