{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAH770GMcFcy"
   },
   "source": [
    "## Project Overview\n",
    "\n",
    "### Problem Statement\n",
    "In today's digital world, understanding how Reddit communities function is crucial for moderators, users, and researchers...\n",
    "\n",
    "### Data Collection Overview\n",
    "- **Tools**:\n",
    "  - PRAW (Python Reddit API Wrapper)\n",
    "  - BeautifulSoup / Scrapy\n",
    "  \n",
    "- **Data Points to Collect**:\n",
    "  - **Posts**: Title, content, upvotes...\n",
    "  - **Comments**: Content, upvotes...\n",
    "  \n",
    "### Solution Approach\n",
    "1. **Sentiment & Engagement Analysis**\n",
    "   - Visualizations using Matplotlib and Seaborn...\n",
    "   \n",
    "2. **Correlation Analysis**\n",
    "   - Apply classification algorithms using scikit-learn...\n",
    "\n",
    "### Expected Deliverables\n",
    "- Insight Report\n",
    "- Actionable Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1xAJ3tSeEx6"
   },
   "source": [
    "## Setup Environment\n",
    "\n",
    "### Purpose\n",
    "This section prepares our Google Colab environment for the Reddit Communities analysis project as outlined in our team's proposal. We'll install the necessary Python libraries to handle data collection, processing, analysis, and visualization.\n",
    "\n",
    "### Key Libraries\n",
    "- PRAW: For accessing the Reddit API\n",
    "- pandas: For data manipulation and analysis\n",
    "- numpy: For numerical computing\n",
    "- matplotlib and seaborn: For data visualization\n",
    "- nltk: For natural language processing and sentiment analysis\n",
    "- scikit-learn: For machine learning tasks\n",
    "\n",
    "### Alignment with Project Goals\n",
    "These libraries support our objectives of:\n",
    "1. Analyzing moderation strategies\n",
    "2. Predicting post impact\n",
    "3. Visualizing Reddit community interactions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup and Installation\n",
    "Make sure to install all necessary libraries first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MCitsxPEa-Kb",
    "outputId": "5c380300-ea61-4a94-e37a-c18e3f3d1fe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in c:\\users\\karth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (7.8.1)\n",
      "Requirement already satisfied: prawcore<3,>=2.4 in c:\\users\\karth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from praw) (2.4.0)\n",
      "Requirement already satisfied: update_checker>=0.18 in c:\\users\\karth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\karth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\karth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\karth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\karth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\karth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\karth\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install praw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import praw  # This is the Reddit API wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Initialize Reddit API\n",
    "Define and call a function to authenticate with the Reddit API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "def setup_reddit_api():\n",
    "    return praw.Reddit(\n",
    "        client_id=\"nrakGjG_wnBE_5UdcHNJoQ\",\n",
    "        client_secret=\"qmGr1q_4pGIBR0pYJE8cyhUbTbdX2w\",\n",
    "        user_agent=\"LittleCheesyExplorers/1.0 (Reddit Communities Analysis Project)\"\n",
    "    )\n",
    "\n",
    "reddit = setup_reddit_api()\n",
    "\n",
    "print(reddit.user.me())  # To test if the Reddit API connection is successful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Load Read required Subreddits \n",
    "This code block will store the names of the subreddits that we want to collect data from.\n",
    "\n",
    "The names will be stored in a text file and we will read from that and scrape based on that list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WritingPrompts', 'TrueOffMyChest', 'NoSleep', 'ExplainLikeImFive', 'IAmA', 'CasualConversation', 'TrueAskReddit', 'Confession', 'relationships', 'ShortScaryStories', 'ProRevenge', 'NuclearRevenge', 'LifeProTips', 'needadvice', 'TrueUnpopularOpinion']\n"
     ]
    }
   ],
   "source": [
    "with open('subreddits.txt', 'r') as file:\n",
    "    subreddit_list = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "print(subreddit_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Functions to Collect Data from Reddit\n",
    "Define functions to collect posts, comments, and subreddit-level data. This is separated for modularity and ease of testing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Collect Posts from a Subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_subreddit_posts(subreddit_name, post_limit=10):\n",
    "    # Collect posts from a single subreddit\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts_data = []\n",
    "\n",
    "    for post in subreddit.hot(limit=post_limit):\n",
    "        posts_data.append({\n",
    "            'subreddit': subreddit_name,\n",
    "            'title': post.title,\n",
    "            'content': post.selftext,\n",
    "            'upvotes': post.score,\n",
    "            'upvote_ratio': post.upvote_ratio,\n",
    "            'comments_count': post.num_comments,\n",
    "            'author': post.author.name if post.author else '[deleted]',\n",
    "            'timestamp': datetime.fromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'post_id': post.id\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(posts_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Collect Data from Multiple Subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_posts_from_subreddits(subreddit_list, post_limit=10):\n",
    "    all_posts = []\n",
    "\n",
    "    for subreddit_name in subreddit_list:\n",
    "        print(f\"Collecting posts from r/{subreddit_name}\")\n",
    "        try:\n",
    "            posts_df = collect_subreddit_posts(subreddit_name, post_limit)\n",
    "            all_posts.append(posts_df)\n",
    "            print(f\"Collected {len(posts_df)} posts from r/{subreddit_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error collecting posts from r/{subreddit_name}: {str(e)}\")\n",
    "\n",
    "    combined_df = pd.concat(all_posts, ignore_index=True)\n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Collect Subreddit-Level Data (Moderators, Rules, Subscriber Counts)\n",
    "Functions to collect metadata for each subreddit, including subscriber count, rules, and moderator counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_subreddit_level_data(reddit, subreddits, limit=10): \n",
    "    # Collect data at the subreddit level (e.g., subscriber count, rules)\n",
    "    subreddit_level_data = []\n",
    "\n",
    "    for subreddit_name in subreddits[:limit]:\n",
    "        try:\n",
    "            subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "            subscriber_count = subreddit.subscribers\n",
    "\n",
    "            try:\n",
    "                rules = list(subreddit.rules())\n",
    "                num_rules = len(rules)\n",
    "                rule_severity = [rule.severity for rule in rules]\n",
    "            except Exception as rule_error:\n",
    "                num_rules = 0\n",
    "                rule_severity = []\n",
    "                print(f\"Could not fetch rules for r/{subreddit_name}: {rule_error}\")\n",
    "\n",
    "            try:\n",
    "                moderators = len(list(subreddit.moderators()))\n",
    "            except Exception as mod_error:\n",
    "                moderators = 0\n",
    "                print(f\"Could not fetch moderators for r/{subreddit_name}: {mod_error}\")\n",
    "\n",
    "            subreddit_data = {\n",
    "                \"subreddit_name\": subreddit_name,\n",
    "                \"subscriber_count\": subscriber_count,\n",
    "                \"num_rules\": num_rules,\n",
    "                \"moderator_count\": moderators,\n",
    "                \"rule_severity\": rule_severity\n",
    "            }\n",
    "            subreddit_level_data.append(subreddit_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for subreddit {subreddit_name}: {e}\")\n",
    "\n",
    "    return subreddit_level_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Collect Post Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' def collect_comments(post_id, comment_limit=5):\\n    # Collect comments for a single post\\n    comments_data = []\\n    try:\\n        submission = reddit.submission(id=post_id)\\n        submission.comments.replace_more(limit=0)\\n        for comment in submission.comments.list()[:comment_limit]:\\n            comments_data.append({\\n                \\'post_id\\': post_id,\\n                \\'comment_id\\': comment.id,\\n                \\'content\\': comment.body,\\n                \\'upvotes\\': comment.score,\\n                \\'author\\': comment.author.name if comment.author else \\'[deleted]\\',\\n                \\'timestamp\\': datetime.fromtimestamp(comment.created_utc).strftime(\\'%Y-%m-%d %H:%M:%S\\')\\n            })\\n    except Exception as e:\\n        print(f\"Error collecting comments for post {post_id}: {str(e)}\")\\n\\n    return comments_data\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" def collect_comments(post_id, comment_limit=5):\n",
    "    # Collect comments for a single post\n",
    "    comments_data = []\n",
    "    try:\n",
    "        submission = reddit.submission(id=post_id)\n",
    "        submission.comments.replace_more(limit=0)\n",
    "        for comment in submission.comments.list()[:comment_limit]:\n",
    "            comments_data.append({\n",
    "                'post_id': post_id,\n",
    "                'comment_id': comment.id,\n",
    "                'content': comment.body,\n",
    "                'upvotes': comment.score,\n",
    "                'author': comment.author.name if comment.author else '[deleted]',\n",
    "                'timestamp': datetime.fromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Error collecting comments for post {post_id}: {str(e)}\")\n",
    "\n",
    "    return comments_data\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6  Fetch Top-Level Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' def collect_top_level_comments(post_id, comment_limit=5):\\n    # Collect top-level comments for a single post\\n    comments_data = []\\n    try:\\n        submission = reddit.submission(id=post_id)\\n        submission.comments.replace_more(limit=0)\\n        for comment in submission.comments[:comment_limit]:\\n            comments_data.append({\\n                \\'post_id\\': post_id,\\n                \\'comment_id\\': comment.id,\\n                \\'content\\': comment.body,\\n                \\'upvotes\\': comment.score,\\n                \\'author\\': comment.author.name if comment.author else \\'[deleted]\\',\\n                \\'timestamp\\': datetime.fromtimestamp(comment.created_utc).strftime(\\'%Y-%m-%d %H:%M:%S\\')\\n            })\\n    except Exception as e:\\n        print(f\"Error collecting comments for post {post_id}: {str(e)}\")\\n\\n    return comments_data\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" def collect_top_level_comments(post_id, comment_limit=5):\n",
    "    # Collect top-level comments for a single post\n",
    "    comments_data = []\n",
    "    try:\n",
    "        submission = reddit.submission(id=post_id)\n",
    "        submission.comments.replace_more(limit=0)\n",
    "        for comment in submission.comments[:comment_limit]:\n",
    "            comments_data.append({\n",
    "                'post_id': post_id,\n",
    "                'comment_id': comment.id,\n",
    "                'content': comment.body,\n",
    "                'upvotes': comment.score,\n",
    "                'author': comment.author.name if comment.author else '[deleted]',\n",
    "                'timestamp': datetime.fromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Error collecting comments for post {post_id}: {str(e)}\")\n",
    "\n",
    "    return comments_data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Analyze and Label Engagement for Posts\n",
    "Calculates engagement scores and labels posts with engagement levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_engagement(post):\n",
    "    upvotes = post['upvotes']\n",
    "    comments_count = post['comments_count']\n",
    "    subscribers = post['subscriber_count']\n",
    "    \n",
    "    if subscribers > 0:\n",
    "        engagement = (upvotes + comments_count) / subscribers\n",
    "    else:\n",
    "        engagement = 0  \n",
    "    return engagement\n",
    "\n",
    "def define_engagement(posts_df):\n",
    "    def label_engagement(score):\n",
    "        if score < 0.0025:\n",
    "            return \"Low\"\n",
    "        elif score < 0.0050:\n",
    "            return \"Medium\"\n",
    "        else:\n",
    "            return \"High\"\n",
    "    posts_df['engagement_level'] = posts_df['normalized_engagement'].apply(label_engagement)\n",
    "    return posts_df\n",
    "\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Add Features to Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features_to_posts(df):\n",
    "    # Title and content lengths\n",
    "    df['title_length'] = df['title'].apply(len)\n",
    "    df['post_length'] = df['content'].apply(lambda x: len(str(x)) if pd.notnull(x) else 0)\n",
    "    \n",
    "    # Convert timestamp to datetime format\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "    \n",
    "    # Time of day and day of week\n",
    "    df['time_of_day'] = df['timestamp'].dt.hour\n",
    "    df['day_of_week'] = df['timestamp'].dt.day_name()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Define Engagement Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_engagement(posts_df):\n",
    "    def label_engagement(score):\n",
    "        if score < 1:\n",
    "            return \"Low\"\n",
    "        elif score < 5:\n",
    "            return \"Medium\"\n",
    "        else:\n",
    "            return \"High\"\n",
    "    \n",
    "    posts_df['engagement_level'] = posts_df['engagement'].apply(label_engagement)\n",
    "    return posts_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Main Function to Collect and Label Engagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Collection Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Run the Script\n",
    "Finally, run the main function with your desired subreddit list and post limit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting posts from r/WritingPrompts\n",
      "Collected 975 posts from r/WritingPrompts\n",
      "Collecting posts from r/TrueOffMyChest\n",
      "Collected 904 posts from r/TrueOffMyChest\n",
      "Collecting posts from r/NoSleep\n",
      "Collected 554 posts from r/NoSleep\n",
      "Collecting posts from r/ExplainLikeImFive\n",
      "Collected 328 posts from r/ExplainLikeImFive\n",
      "Collecting posts from r/IAmA\n",
      "Collected 560 posts from r/IAmA\n",
      "Collecting posts from r/CasualConversation\n",
      "Collected 754 posts from r/CasualConversation\n",
      "Collecting posts from r/TrueAskReddit\n",
      "Collected 225 posts from r/TrueAskReddit\n",
      "Collecting posts from r/Confession\n",
      "Collected 177 posts from r/Confession\n",
      "Collecting posts from r/relationships\n",
      "Collected 250 posts from r/relationships\n",
      "Collecting posts from r/ShortScaryStories\n",
      "Collected 979 posts from r/ShortScaryStories\n",
      "Collecting posts from r/ProRevenge\n",
      "Collected 44 posts from r/ProRevenge\n",
      "Collecting posts from r/NuclearRevenge\n",
      "Collected 423 posts from r/NuclearRevenge\n",
      "Collecting posts from r/LifeProTips\n",
      "Collected 355 posts from r/LifeProTips\n",
      "Collecting posts from r/needadvice\n",
      "Collected 338 posts from r/needadvice\n",
      "Collecting posts from r/TrueUnpopularOpinion\n",
      "Collected 881 posts from r/TrueUnpopularOpinion\n",
      "\n",
      "All posts data saved to subreddit_posts.csv\n",
      "\n",
      "Summary of collected posts:\n",
      "subreddit\n",
      "ShortScaryStories       979\n",
      "WritingPrompts          975\n",
      "TrueOffMyChest          904\n",
      "TrueUnpopularOpinion    881\n",
      "CasualConversation      754\n",
      "IAmA                    560\n",
      "NoSleep                 554\n",
      "NuclearRevenge          423\n",
      "LifeProTips             355\n",
      "needadvice              338\n",
      "ExplainLikeImFive       328\n",
      "relationships           250\n",
      "TrueAskReddit           225\n",
      "Confession              177\n",
      "ProRevenge               44\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\AppData\\Local\\Temp\\ipykernel_16900\\2042881739.py:12: DeprecationWarning: Calling SubredditRules to get a list of rules is deprecated. Remove the parentheses to use the iterator. View the PRAW documentation on how to change the code in order to use the iterator (https://praw.readthedocs.io/en/latest/code_overview/other/subredditrules.html#praw.models.reddit.rules.SubredditRules.__call__).\n",
      "  rules = list(subreddit.rules())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not fetch rules for r/WritingPrompts: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/WritingPrompts: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/TrueOffMyChest: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/TrueOffMyChest: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/NoSleep: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/NoSleep: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/ExplainLikeImFive: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/ExplainLikeImFive: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/IAmA: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/IAmA: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/CasualConversation: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/CasualConversation: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/TrueAskReddit: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/TrueAskReddit: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/Confession: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/Confession: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/relationships: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/relationships: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/ShortScaryStories: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/ShortScaryStories: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/ProRevenge: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/ProRevenge: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/NuclearRevenge: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/NuclearRevenge: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/LifeProTips: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/LifeProTips: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/needadvice: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/needadvice: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/TrueUnpopularOpinion: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/TrueUnpopularOpinion: 'Subreddit' object has no attribute 'moderators'\n",
      "\n",
      "Subreddit-level data saved to subreddit_level_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Collect posts from all subreddits\n",
    "all_posts_df = collect_posts_from_subreddits(subreddit_list, post_limit=10000)\n",
    "\n",
    "# Save all collected posts to a single CSV file\n",
    "csv_filename = \"subreddit_posts.csv\"\n",
    "all_posts_df.to_csv(csv_filename, index=False)\n",
    "print(f\"\\nAll posts data saved to {csv_filename}\")\n",
    "\n",
    "# Print summary of collected posts\n",
    "print(\"\\nSummary of collected posts:\")\n",
    "print(all_posts_df['subreddit'].value_counts())\n",
    "\n",
    "# Collect subreddit-level data (e.g., rules, moderation, etc.)\n",
    "subreddit_level_data = collect_subreddit_level_data(reddit, subreddit_list, limit=5000)\n",
    "\n",
    "# Convert to DataFrame and save to CSV\n",
    "subreddit_level_df = pd.DataFrame(subreddit_level_data)\n",
    "subreddit_level_df.to_csv(\"subreddit_level_data.csv\", index=False)\n",
    "print(\"\\nSubreddit-level data saved to subreddit_level_data.csv\")\n",
    "\n",
    "\n",
    "all_posts_df = all_posts_df.merge(subreddit_level_df[['subreddit_name', 'subscriber_count']], \n",
    "                          left_on='subreddit', right_on='subreddit_name', how='left')\n",
    "all_posts_df['engagement'] = all_posts_df.apply(calculate_engagement, axis=1)\n",
    "all_posts_df['normalized_engagement'] = all_posts_df['engagement'] * 10000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL CLEANED CSV Creation and Storing\n",
    "In this Code Block we will be creating a csv file and storing all our scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
