{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAH770GMcFcy"
   },
   "source": [
    "## Project Overview\n",
    "\n",
    "### Problem Statement\n",
    "In today's digital world, understanding how Reddit communities function is crucial for moderators, users, and researchers...\n",
    "\n",
    "### Data Collection Overview\n",
    "- **Tools**:\n",
    "  - PRAW (Python Reddit API Wrapper)\n",
    "  - BeautifulSoup / Scrapy\n",
    "  \n",
    "- **Data Points to Collect**:\n",
    "  - **Posts**: Title, content, upvotes...\n",
    "  - **Comments**: Content, upvotes...\n",
    "  \n",
    "### Solution Approach\n",
    "1. **Sentiment & Engagement Analysis**\n",
    "   - Visualizations using Matplotlib and Seaborn...\n",
    "   \n",
    "2. **Correlation Analysis**\n",
    "   - Apply classification algorithms using scikit-learn...\n",
    "\n",
    "### Expected Deliverables\n",
    "- Insight Report\n",
    "- Actionable Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1xAJ3tSeEx6"
   },
   "source": [
    "## Setup Environment\n",
    "\n",
    "### Purpose\n",
    "This section prepares our Google Colab environment for the Reddit Communities analysis project as outlined in our team's proposal. We'll install the necessary Python libraries to handle data collection, processing, analysis, and visualization.\n",
    "\n",
    "### Key Libraries\n",
    "- PRAW: For accessing the Reddit API\n",
    "- pandas: For data manipulation and analysis\n",
    "- numpy: For numerical computing\n",
    "- matplotlib and seaborn: For data visualization\n",
    "- nltk: For natural language processing and sentiment analysis\n",
    "- scikit-learn: For machine learning tasks\n",
    "\n",
    "### Alignment with Project Goals\n",
    "These libraries support our objectives of:\n",
    "1. Analyzing moderation strategies\n",
    "2. Predicting post impact\n",
    "3. Visualizing Reddit community interactions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup and Installation\n",
    "Make sure to install all necessary libraries first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MCitsxPEa-Kb",
    "outputId": "5c380300-ea61-4a94-e37a-c18e3f3d1fe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (7.8.1)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: prawcore<3,>=2.4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from praw) (2.4.0)\n",
      "Requirement already satisfied: update_checker>=0.18 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "%pip install praw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import praw  \n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Initialize Reddit API\n",
    "Define and call a function to authenticate with the Reddit API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "def setup_reddit_api():\n",
    "    return praw.Reddit(\n",
    "        client_id=\"nrakGjG_wnBE_5UdcHNJoQ\",\n",
    "        client_secret=\"qmGr1q_4pGIBR0pYJE8cyhUbTbdX2w\",\n",
    "        user_agent=\"LittleCheesyExplorers/1.0 (Reddit Communities Analysis Project)\"\n",
    "    )\n",
    "\n",
    "reddit = setup_reddit_api()\n",
    "\n",
    "print(reddit.user.me())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Load Read required Subreddits \n",
    "This code block will store the names of the subreddits that we want to collect data from.\n",
    "\n",
    "The names will be stored in a text file and we will read from that and scrape based on that list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AskReddit', 'ChangeMyView', 'TodayILearned', 'self', 'offmychest', 'Showerthoughts', 'personalfinance', 'AskScience', 'Writing', 'Advice', 'LetsNotMeet', 'SelfImprovement', 'DecidingToBeBetter', 'AskHistorians', 'TwoXChromosomes', 'CasualConversation', 'InternetIsBeautiful', 'nosleep', 'WritingPrompts', 'ExplainLikeImFive', 'TrueOffMyChest', 'UnpopularOpinion', 'relationships', 'TrueAskReddit', 'Confession', 'ShortScaryStories', 'ProRevenge', 'NuclearRevenge', 'LifeProTips', 'needadvice', 'TrueUnpopularOpinion']\n"
     ]
    }
   ],
   "source": [
    "with open('subreddits.txt', 'r') as file:\n",
    "    subreddit_list = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "print(subreddit_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Functions to Collect Data from Reddit\n",
    "Define functions to collect posts, comments, and subreddit-level data. This is separated for modularity and ease of testing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Collect Posts from a Subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_subreddit_posts(subreddit_name, post_limit=10):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts_data = []\n",
    "\n",
    "    for post in subreddit.hot(limit=post_limit):\n",
    "        posts_data.append({\n",
    "            'subreddit': subreddit_name,\n",
    "            'title': post.title,\n",
    "            'content': post.selftext,\n",
    "            'upvotes': post.score,\n",
    "            'upvote_ratio': post.upvote_ratio,\n",
    "            'comments_count': post.num_comments,\n",
    "            'author': post.author.name if post.author else '[deleted]',\n",
    "            'timestamp': datetime.fromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'post_id': post.id\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(posts_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Collect Data from Multiple Subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_posts_from_subreddits(subreddit_list, post_limit=10):\n",
    "    all_posts = []\n",
    "\n",
    "    for subreddit_name in subreddit_list:\n",
    "        print(f\"Collecting posts from r/{subreddit_name}\")\n",
    "        try:\n",
    "            posts_df = collect_subreddit_posts(subreddit_name, post_limit)\n",
    "            all_posts.append(posts_df)\n",
    "            print(f\"Collected {len(posts_df)} posts from r/{subreddit_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error collecting posts from r/{subreddit_name}: {str(e)}\")\n",
    "\n",
    "    combined_df = pd.concat(all_posts, ignore_index=True)\n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Collect Subreddit-Level Data (Moderators, Rules, Subscriber Counts)\n",
    "Functions to collect metadata for each subreddit, including subscriber count, rules, and moderator counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_subreddit_level_data(reddit, subreddits, limit=10): \n",
    "    subreddit_level_data = []\n",
    "\n",
    "    for subreddit_name in subreddits[:limit]:\n",
    "        try:\n",
    "            subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "            subscriber_count = subreddit.subscribers\n",
    "\n",
    "            try:\n",
    "                rules = list(subreddit.rules())\n",
    "                num_rules = len(rules)\n",
    "                rule_severity = [rule.severity for rule in rules]\n",
    "            except Exception as rule_error:\n",
    "                num_rules = 0\n",
    "                rule_severity = []\n",
    "                print(f\"Could not fetch rules for r/{subreddit_name}: {rule_error}\")\n",
    "\n",
    "            try:\n",
    "                moderators = len(list(subreddit.moderators()))\n",
    "            except Exception as mod_error:\n",
    "                moderators = 0\n",
    "                print(f\"Could not fetch moderators for r/{subreddit_name}: {mod_error}\")\n",
    "\n",
    "            subreddit_data = {\n",
    "                \"subreddit_name\": subreddit_name,\n",
    "                \"subscriber_count\": subscriber_count,\n",
    "                \"num_rules\": num_rules,\n",
    "                \"moderator_count\": moderators,\n",
    "                \"rule_severity\": rule_severity\n",
    "            }\n",
    "            subreddit_level_data.append(subreddit_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for subreddit {subreddit_name}: {e}\")\n",
    "\n",
    "    return subreddit_level_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Comments method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    # Read the post IDs and other data from the provided CSV file\\n    posts_df = pd.read_csv(csv_file)\\n    comments_data = []\\n    \\n    for _, row in posts_df.iterrows():\\n        post_id = row[\\'post_id\\']\\n        subreddit_name = row[\\'subreddit\\']\\n        \\n        try:\\n            # Fetch the post using the post_id\\n            post = reddit.submission(id=post_id)\\n            \\n            # Replace \"MoreComments\" with empty (expand all top-level comments)\\n            post.comments.replace_more(limit=0)\\n            \\n            # Iterate through top-level comments\\n            top_comments = [comment for comment in post.comments if isinstance(comment, praw.models.Comment)]\\n            \\n            # Limit the number of comments to comment_limit\\n            for comment in top_comments[:comment_limit]:  \\n                comments_data.append({\\n                    \\'subreddit\\': subreddit_name,\\n                    \\'comment\\': comment.body,\\n                    \\'comment_author\\': comment.author.name if comment.author else \\'[deleted]\\',\\n                    \\'author_karma\\': (comment.author.link_karma + comment.author.comment_karma) if comment.author else 0,\\n                    \\'post_title\\': post.title,\\n                    \\'post_content\\': post.selftext,\\n                    \\'post_upvotes\\': post.score,\\n                    \\'timestamp\\': datetime.fromtimestamp(comment.created_utc).strftime(\\'%Y-%m-%d %H:%M:%S\\'),\\n                })\\n            \\n            print(f\"Collected {len(top_comments[:comment_limit])} comments from post {post_id}\")\\n        \\n        except Exception as e:\\n            print(f\"Error collecting comments from post {post_id}: {e}\")\\n        \\n        # Optional: Small delay to avoid hitting rate limits\\n        time.sleep(1) \\n    \\n    return pd.DataFrame(comments_data)\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def collect_top_level_comments_from_csv(csv_file, comment_limit=2):\n",
    "    # Read posts from CSV file\n",
    "    posts_df = pd.read_csv(csv_file)\n",
    "    comments_data = []\n",
    "\n",
    "    # Iterate through each post in the CSV file\n",
    "    for _, row in posts_df.iterrows():\n",
    "        post_id = row['post_id']  # 'post_id' column from your CSV\n",
    "        subreddit_name = row['subreddit']  # 'subreddit' column from your CSV\n",
    "        \n",
    "        try:\n",
    "            # Fetch the post using the post ID\n",
    "            post = reddit.submission(id=post_id)\n",
    "            post.comments.replace_more(limit=0)  # Ensure all top-level comments are loaded\n",
    "            \n",
    "            # Debug: Print post details to confirm it's being processed\n",
    "            print(f\"Processing post {post_id} in subreddit {subreddit_name}\")\n",
    "            \n",
    "            # Check if the post has comments\n",
    "            if not post.comments:\n",
    "                print(f\"No comments found for post {post_id}\")\n",
    "                continue\n",
    "\n",
    "            # Collect top-level comments\n",
    "            for comment in post.comments[:comment_limit]:  # Limit to the top `comment_limit` comments\n",
    "                if comment.parent_id.split('_')[1] == post_id:  # Ensure it's a top-level comment\n",
    "                    \n",
    "                    # Get comment author and karma (handle potential missing authors)\n",
    "                    author_karma = 0\n",
    "                    author_name = \"[deleted]\"\n",
    "                    \n",
    "                    if comment.author:\n",
    "                        author_name = comment.author.name\n",
    "                        author_karma = comment.author.link_karma + comment.author.comment_karma  # Total karma\n",
    "\n",
    "                    comments_data.append({\n",
    "                        'subreddit': subreddit_name,\n",
    "                        'comment': comment.body,\n",
    "                        'comment_author': author_name,\n",
    "                        'author_karma': author_karma,\n",
    "                        'post_title': post.title,\n",
    "                        'post_content': post.selftext,\n",
    "                        'post_upvotes': post.score,\n",
    "                        'timestamp': datetime.fromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    })\n",
    "            \n",
    "            print(f\"Collected {len(comments_data)} comments from post {post_id}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error collecting comments from post {post_id}: {e}\")\n",
    "        \n",
    "        # Optional: Small delay to avoid hitting rate limits\n",
    "        time.sleep(1) \n",
    "    \n",
    "    # Return the DataFrame of collected comments\n",
    "    return pd.DataFrame(comments_data)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def collect_top_level_comments(csv_file, comment_limit=2):\n",
    "    # Read the CSV containing post information\n",
    "    posts_df = pd.read_csv(csv_file)\n",
    "    comments_data = []\n",
    "\n",
    "    # Loop through the DataFrame rows and extract post_id and subreddit\n",
    "    for _, row in posts_df.iterrows():\n",
    "        post_id = row['post_id']  # Ensure the CSV has a 'post_id' column\n",
    "        subreddit_name = row['subreddit']  # Ensure the CSV has a 'subreddit' column\n",
    "\n",
    "        try:\n",
    "            # Get the post by its ID\n",
    "            post = reddit.submission(id=post_id)\n",
    "            post.comments.replace_more(limit=0)  # Ensure we get all comments\n",
    "\n",
    "            # Debugging: Print post details to ensure it's fetched\n",
    "            print(f\"Processing post {post_id} from subreddit {subreddit_name}\")\n",
    "\n",
    "            # Collect top-level comments (limit to 'comment_limit' comments per post)\n",
    "            for comment in post.comments[:comment_limit]:  # Limiting the number of comments\n",
    "                if comment.parent_id.split('_')[1] == post_id:  # Check if it's a top-level comment\n",
    "                    # Initialize author details\n",
    "                    author_karma = 0\n",
    "                    author_name = \"[deleted]\"\n",
    "                    \n",
    "                    if comment.author:\n",
    "                        author_name = comment.author.name\n",
    "                        author_karma = comment.author.link_karma + comment.author.comment_karma  # Total karma\n",
    "\n",
    "                    # Add the comment data to the list\n",
    "                    comments_data.append({\n",
    "                        'subreddit': subreddit_name,\n",
    "                        'comment': comment.body,\n",
    "                        'comment_author': author_name,\n",
    "                        'author_karma': author_karma,\n",
    "                        'post_title': post.title,\n",
    "                        'post_content': post.selftext,\n",
    "                        'post_upvotes': post.score,\n",
    "                        'timestamp': datetime.fromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    })\n",
    "\n",
    "            print(f\"Collected {len(comments_data)} comments from post {post_id}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error collecting comments from post {post_id}: {e}\")\n",
    "        \n",
    "        # Small delay to avoid Reddit API rate limits\n",
    "        time.sleep(1) \n",
    "    \n",
    "    # Return the collected comments as a DataFrame\n",
    "    return pd.DataFrame(comments_data)\n",
    "\n",
    "def save_comments_to_csv(csv_file, comment_limit=2):\n",
    "    comments_df = collect_top_level_comments(csv_file, comment_limit)\n",
    "    \n",
    "    # Check if the DataFrame has data\n",
    "    if comments_df.empty:\n",
    "        print(\"No comments collected.\")\n",
    "    else:\n",
    "        comments_df.to_csv(\"top_level_comments.csv\", index=False)\n",
    "        print(\"\\nComments saved to collected_comments_with_author_and_karma.csv\")\n",
    "\n",
    "\"\"\"\n",
    "    # Read the post IDs and other data from the provided CSV file\n",
    "    posts_df = pd.read_csv(csv_file)\n",
    "    comments_data = []\n",
    "    \n",
    "    for _, row in posts_df.iterrows():\n",
    "        post_id = row['post_id']\n",
    "        subreddit_name = row['subreddit']\n",
    "        \n",
    "        try:\n",
    "            # Fetch the post using the post_id\n",
    "            post = reddit.submission(id=post_id)\n",
    "            \n",
    "            # Replace \"MoreComments\" with empty (expand all top-level comments)\n",
    "            post.comments.replace_more(limit=0)\n",
    "            \n",
    "            # Iterate through top-level comments\n",
    "            top_comments = [comment for comment in post.comments if isinstance(comment, praw.models.Comment)]\n",
    "            \n",
    "            # Limit the number of comments to comment_limit\n",
    "            for comment in top_comments[:comment_limit]:  \n",
    "                comments_data.append({\n",
    "                    'subreddit': subreddit_name,\n",
    "                    'comment': comment.body,\n",
    "                    'comment_author': comment.author.name if comment.author else '[deleted]',\n",
    "                    'author_karma': (comment.author.link_karma + comment.author.comment_karma) if comment.author else 0,\n",
    "                    'post_title': post.title,\n",
    "                    'post_content': post.selftext,\n",
    "                    'post_upvotes': post.score,\n",
    "                    'timestamp': datetime.fromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                })\n",
    "            \n",
    "            print(f\"Collected {len(top_comments[:comment_limit])} comments from post {post_id}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error collecting comments from post {post_id}: {e}\")\n",
    "        \n",
    "        # Optional: Small delay to avoid hitting rate limits\n",
    "        time.sleep(1) \n",
    "    \n",
    "    return pd.DataFrame(comments_data)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Example usage\n",
    "# save_comments_to_csv(\"reddit_posts.csv\", comment_limit=2)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Analyze and Label Engagement for Posts\n",
    "Calculates engagement scores and labels posts with engagement levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_engagement(post):\n",
    "    upvotes = post['upvotes']\n",
    "    comments_count = post['comments_count']\n",
    "    subscribers = post['subscriber_count']\n",
    "    \n",
    "    if subscribers > 0:\n",
    "        engagement = (upvotes + comments_count) / subscribers\n",
    "    else:\n",
    "        engagement = 0  \n",
    "    return engagement\n",
    "\n",
    "def define_engagement(posts_df):\n",
    "    def label_engagement(score):\n",
    "        if score < 0.0025:\n",
    "            return \"Low\"\n",
    "        elif score < 0.0050:\n",
    "            return \"Medium\"\n",
    "        else:\n",
    "            return \"High\"\n",
    "    posts_df['engagement_level'] = posts_df['normalized_engagement'].apply(label_engagement)\n",
    "    return posts_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Define Engagement Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_engagement(posts_df):\n",
    "    def label_engagement(score):\n",
    "        if score < 1:\n",
    "            return \"Low\"\n",
    "        elif score < 5:\n",
    "            return \"Medium\"\n",
    "        else:\n",
    "            return \"High\"\n",
    "    \n",
    "    posts_df['engagement_level'] = posts_df['normalized_engagement'].apply(label_engagement)\n",
    "    return posts_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Add Features to Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features_to_posts(df):\n",
    "    df['title_length'] = df['title'].apply(len)\n",
    "    df['post_length'] = df['content'].apply(lambda x: len(str(x)) if pd.notnull(x) else 0)\n",
    "    \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "    \n",
    "    df['time_of_day'] = df['timestamp'].dt.hour\n",
    "    df['day_of_week'] = df['timestamp'].dt.day_name()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Running All Methods\n",
    "Finally, running all the methods in order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting posts from r/AskReddit\n",
      "Collected 4 posts from r/AskReddit\n",
      "Collecting posts from r/ChangeMyView\n",
      "Collected 4 posts from r/ChangeMyView\n",
      "Collecting posts from r/TodayILearned\n",
      "Collected 4 posts from r/TodayILearned\n",
      "Collecting posts from r/self\n",
      "Collected 4 posts from r/self\n",
      "Collecting posts from r/offmychest\n",
      "Collected 4 posts from r/offmychest\n",
      "Collecting posts from r/Showerthoughts\n",
      "Collected 4 posts from r/Showerthoughts\n",
      "Collecting posts from r/personalfinance\n",
      "Collected 4 posts from r/personalfinance\n",
      "Collecting posts from r/AskScience\n",
      "Collected 4 posts from r/AskScience\n",
      "Collecting posts from r/Writing\n",
      "Collected 4 posts from r/Writing\n",
      "Collecting posts from r/Advice\n",
      "Collected 4 posts from r/Advice\n",
      "Collecting posts from r/LetsNotMeet\n",
      "Collected 4 posts from r/LetsNotMeet\n",
      "Collecting posts from r/SelfImprovement\n",
      "Collected 4 posts from r/SelfImprovement\n",
      "Collecting posts from r/DecidingToBeBetter\n",
      "Collected 4 posts from r/DecidingToBeBetter\n",
      "Collecting posts from r/AskHistorians\n",
      "Collected 4 posts from r/AskHistorians\n",
      "Collecting posts from r/TwoXChromosomes\n",
      "Collected 4 posts from r/TwoXChromosomes\n",
      "Collecting posts from r/CasualConversation\n",
      "Collected 4 posts from r/CasualConversation\n",
      "Collecting posts from r/InternetIsBeautiful\n",
      "Collected 4 posts from r/InternetIsBeautiful\n",
      "Collecting posts from r/nosleep\n",
      "Collected 4 posts from r/nosleep\n",
      "Collecting posts from r/WritingPrompts\n",
      "Collected 4 posts from r/WritingPrompts\n",
      "Collecting posts from r/ExplainLikeImFive\n",
      "Collected 4 posts from r/ExplainLikeImFive\n",
      "Collecting posts from r/TrueOffMyChest\n",
      "Collected 4 posts from r/TrueOffMyChest\n",
      "Collecting posts from r/UnpopularOpinion\n",
      "Collected 4 posts from r/UnpopularOpinion\n",
      "Collecting posts from r/relationships\n",
      "Collected 4 posts from r/relationships\n",
      "Collecting posts from r/TrueAskReddit\n",
      "Collected 4 posts from r/TrueAskReddit\n",
      "Collecting posts from r/Confession\n",
      "Collected 4 posts from r/Confession\n",
      "Collecting posts from r/ShortScaryStories\n",
      "Collected 4 posts from r/ShortScaryStories\n",
      "Collecting posts from r/ProRevenge\n",
      "Collected 4 posts from r/ProRevenge\n",
      "Collecting posts from r/NuclearRevenge\n",
      "Collected 4 posts from r/NuclearRevenge\n",
      "Collecting posts from r/LifeProTips\n",
      "Collected 4 posts from r/LifeProTips\n",
      "Collecting posts from r/needadvice\n",
      "Collected 4 posts from r/needadvice\n",
      "Collecting posts from r/TrueUnpopularOpinion\n",
      "Collected 4 posts from r/TrueUnpopularOpinion\n",
      "\n",
      "All posts data saved to subreddit_posts.csv\n",
      "\n",
      "Summary of collected posts:\n",
      "subreddit\n",
      "AskReddit               4\n",
      "ChangeMyView            4\n",
      "TodayILearned           4\n",
      "self                    4\n",
      "offmychest              4\n",
      "Showerthoughts          4\n",
      "personalfinance         4\n",
      "AskScience              4\n",
      "Writing                 4\n",
      "Advice                  4\n",
      "LetsNotMeet             4\n",
      "SelfImprovement         4\n",
      "DecidingToBeBetter      4\n",
      "AskHistorians           4\n",
      "TwoXChromosomes         4\n",
      "CasualConversation      4\n",
      "InternetIsBeautiful     4\n",
      "nosleep                 4\n",
      "WritingPrompts          4\n",
      "ExplainLikeImFive       4\n",
      "TrueOffMyChest          4\n",
      "UnpopularOpinion        4\n",
      "relationships           4\n",
      "TrueAskReddit           4\n",
      "Confession              4\n",
      "ShortScaryStories       4\n",
      "ProRevenge              4\n",
      "NuclearRevenge          4\n",
      "LifeProTips             4\n",
      "needadvice              4\n",
      "TrueUnpopularOpinion    4\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_20576\\3403129084.py:11: DeprecationWarning: Calling SubredditRules to get a list of rules is deprecated. Remove the parentheses to use the iterator. View the PRAW documentation on how to change the code in order to use the iterator (https://praw.readthedocs.io/en/latest/code_overview/other/subredditrules.html#praw.models.reddit.rules.SubredditRules.__call__).\n",
      "  rules = list(subreddit.rules())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not fetch rules for r/AskReddit: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/AskReddit: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/ChangeMyView: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/ChangeMyView: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/TodayILearned: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/TodayILearned: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/self: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/self: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/offmychest: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/offmychest: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/Showerthoughts: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/Showerthoughts: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/personalfinance: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/personalfinance: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/AskScience: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/AskScience: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/Writing: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/Writing: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/Advice: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/Advice: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/LetsNotMeet: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/LetsNotMeet: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/SelfImprovement: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/SelfImprovement: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/DecidingToBeBetter: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/DecidingToBeBetter: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/AskHistorians: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/AskHistorians: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/TwoXChromosomes: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/TwoXChromosomes: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/CasualConversation: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/CasualConversation: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/InternetIsBeautiful: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/InternetIsBeautiful: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/nosleep: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/nosleep: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/WritingPrompts: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/WritingPrompts: 'Subreddit' object has no attribute 'moderators'\n",
      "Could not fetch rules for r/ExplainLikeImFive: 'str' object has no attribute 'severity'\n",
      "Could not fetch moderators for r/ExplainLikeImFive: 'Subreddit' object has no attribute 'moderators'\n",
      "\n",
      "Subreddit-level data saved to subreddit_level_data.csv\n",
      "\n",
      "Subreddit posts with engagement scores saved to subreddit_posts_with_engagement.csv\n",
      "Processing post 1gkk9s3 from subreddit AskReddit\n",
      "Collected 2 comments from post 1gkk9s3\n",
      "Processing post 1grik2y from subreddit AskReddit\n",
      "Collected 4 comments from post 1grik2y\n",
      "Processing post 1gr5rox from subreddit AskReddit\n",
      "Collected 6 comments from post 1gr5rox\n",
      "Processing post 1grk6v5 from subreddit AskReddit\n",
      "Collected 8 comments from post 1grk6v5\n",
      "Processing post 1grrhm9 from subreddit ChangeMyView\n",
      "Collected 8 comments from post 1grrhm9\n",
      "Processing post 1grkyyp from subreddit ChangeMyView\n",
      "Collected 10 comments from post 1grkyyp\n",
      "Processing post 1grl5em from subreddit ChangeMyView\n",
      "Collected 12 comments from post 1grl5em\n",
      "Processing post 1grpe59 from subreddit ChangeMyView\n",
      "Collected 14 comments from post 1grpe59\n",
      "Processing post 1grlsyv from subreddit TodayILearned\n",
      "Collected 16 comments from post 1grlsyv\n",
      "Processing post 1grn2a7 from subreddit TodayILearned\n",
      "Collected 18 comments from post 1grn2a7\n",
      "Processing post 1grb832 from subreddit TodayILearned\n",
      "Collected 20 comments from post 1grb832\n",
      "Processing post 1gretal from subreddit TodayILearned\n",
      "Collected 22 comments from post 1gretal\n",
      "Processing post 1gnkpa5 from subreddit self\n",
      "Collected 24 comments from post 1gnkpa5\n",
      "Processing post 1grhyhq from subreddit self\n",
      "Collected 26 comments from post 1grhyhq\n",
      "Processing post 1grk70i from subreddit self\n",
      "Collected 28 comments from post 1grk70i\n",
      "Processing post 1grmzre from subreddit self\n",
      "Collected 30 comments from post 1grmzre\n",
      "Processing post u3ydaq from subreddit offmychest\n",
      "Collected 32 comments from post u3ydaq\n",
      "Processing post 1grk2wk from subreddit offmychest\n",
      "Collected 34 comments from post 1grk2wk\n",
      "Processing post 1grhr6z from subreddit offmychest\n",
      "Collected 36 comments from post 1grhr6z\n",
      "Processing post 1grpimz from subreddit offmychest\n",
      "Collected 38 comments from post 1grpimz\n",
      "Processing post 1grre3b from subreddit Showerthoughts\n",
      "Collected 40 comments from post 1grre3b\n",
      "Processing post 1grd9vu from subreddit Showerthoughts\n",
      "Collected 42 comments from post 1grd9vu\n",
      "Processing post 1gropii from subreddit Showerthoughts\n",
      "Collected 44 comments from post 1gropii\n",
      "Processing post 1gqnqup from subreddit Showerthoughts\n",
      "Collected 46 comments from post 1gqnqup\n",
      "Processing post 1gh4s5b from subreddit personalfinance\n",
      "Collected 48 comments from post 1gh4s5b\n",
      "Processing post 1gopn91 from subreddit personalfinance\n",
      "Collected 50 comments from post 1gopn91\n",
      "Processing post 1groopl from subreddit personalfinance\n",
      "Collected 52 comments from post 1groopl\n",
      "Processing post 1gr9gpc from subreddit personalfinance\n",
      "Collected 54 comments from post 1gr9gpc\n",
      "Processing post 1e73crx from subreddit AskScience\n",
      "Collected 56 comments from post 1e73crx\n",
      "Processing post 1grj9rf from subreddit AskScience\n",
      "Collected 58 comments from post 1grj9rf\n",
      "Processing post 1gr5ux6 from subreddit AskScience\n",
      "Collected 60 comments from post 1gr5ux6\n",
      "Processing post 1gqn95z from subreddit AskScience\n",
      "Collected 62 comments from post 1gqn95z\n",
      "Processing post 1gr5en8 from subreddit Writing\n",
      "Collected 64 comments from post 1gr5en8\n",
      "Processing post 1gmucfd from subreddit Writing\n",
      "Collected 66 comments from post 1gmucfd\n",
      "Processing post 1grsc0u from subreddit Writing\n",
      "Collected 68 comments from post 1grsc0u\n",
      "Processing post 1grnvhg from subreddit Writing\n",
      "Collected 70 comments from post 1grnvhg\n",
      "Processing post 1grdzwh from subreddit Advice\n",
      "Collected 72 comments from post 1grdzwh\n",
      "Processing post 1grnacw from subreddit Advice\n",
      "Collected 74 comments from post 1grnacw\n",
      "Processing post 1grcs3d from subreddit Advice\n",
      "Collected 76 comments from post 1grcs3d\n",
      "Processing post 1gri0yl from subreddit Advice\n",
      "Collected 78 comments from post 1gri0yl\n",
      "Processing post 16t9o8w from subreddit LetsNotMeet\n",
      "Collected 80 comments from post 16t9o8w\n",
      "Processing post 1aviy1c from subreddit LetsNotMeet\n",
      "Collected 82 comments from post 1aviy1c\n",
      "Processing post 1grryt9 from subreddit LetsNotMeet\n",
      "Collected 82 comments from post 1grryt9\n",
      "Processing post 1gqslj1 from subreddit LetsNotMeet\n",
      "Collected 84 comments from post 1gqslj1\n",
      "Processing post 1grom7v from subreddit SelfImprovement\n",
      "Collected 86 comments from post 1grom7v\n",
      "Processing post 1grksq7 from subreddit SelfImprovement\n",
      "Collected 88 comments from post 1grksq7\n",
      "Processing post 1grajgp from subreddit SelfImprovement\n",
      "Collected 90 comments from post 1grajgp\n",
      "Processing post 1grrizr from subreddit SelfImprovement\n",
      "Collected 92 comments from post 1grrizr\n",
      "Processing post cy21ib from subreddit DecidingToBeBetter\n",
      "Collected 94 comments from post cy21ib\n",
      "Processing post 1f9fso0 from subreddit DecidingToBeBetter\n",
      "Error collecting comments from post 1f9fso0: received 404 HTTP response\n",
      "Processing post 1grdx9n from subreddit DecidingToBeBetter\n",
      "Collected 96 comments from post 1grdx9n\n",
      "Processing post 1grn68v from subreddit DecidingToBeBetter\n",
      "Collected 98 comments from post 1grn68v\n",
      "Processing post 1gr5e4c from subreddit AskHistorians\n",
      "Collected 100 comments from post 1gr5e4c\n",
      "Processing post 1gqhmmh from subreddit AskHistorians\n",
      "Collected 102 comments from post 1gqhmmh\n",
      "Processing post 1gre1ad from subreddit AskHistorians\n",
      "Collected 104 comments from post 1gre1ad\n",
      "Processing post 1gri9rk from subreddit AskHistorians\n",
      "Collected 106 comments from post 1gri9rk\n",
      "Processing post fejj7u from subreddit TwoXChromosomes\n",
      "Collected 106 comments from post fejj7u\n",
      "Processing post 1by0725 from subreddit TwoXChromosomes\n",
      "Collected 106 comments from post 1by0725\n",
      "Processing post 1gr8n9t from subreddit TwoXChromosomes\n",
      "Collected 108 comments from post 1gr8n9t\n",
      "Processing post 1grkiv4 from subreddit TwoXChromosomes\n",
      "Collected 110 comments from post 1grkiv4\n",
      "Processing post 1gh7esa from subreddit CasualConversation\n",
      "Collected 112 comments from post 1gh7esa\n",
      "Processing post 1grlo9d from subreddit CasualConversation\n",
      "Collected 114 comments from post 1grlo9d\n",
      "Processing post 1grdykc from subreddit CasualConversation\n",
      "Collected 116 comments from post 1grdykc\n",
      "Processing post 1grqzwo from subreddit CasualConversation\n",
      "Collected 118 comments from post 1grqzwo\n",
      "Processing post 1grmouj from subreddit InternetIsBeautiful\n",
      "Collected 120 comments from post 1grmouj\n",
      "Processing post 1gr1hxo from subreddit InternetIsBeautiful\n",
      "Collected 122 comments from post 1gr1hxo\n",
      "Processing post 1gr2k11 from subreddit InternetIsBeautiful\n",
      "Collected 124 comments from post 1gr2k11\n",
      "Processing post 1gpbpc4 from subreddit InternetIsBeautiful\n",
      "Collected 126 comments from post 1gpbpc4\n",
      "Processing post 1grlz9e from subreddit nosleep\n",
      "Collected 128 comments from post 1grlz9e\n",
      "Processing post 1gra94e from subreddit nosleep\n",
      "Collected 130 comments from post 1gra94e\n",
      "Processing post 1grr5bq from subreddit nosleep\n",
      "Collected 131 comments from post 1grr5bq\n",
      "Processing post 1grl6li from subreddit nosleep\n",
      "Collected 132 comments from post 1grl6li\n",
      "Processing post 1gp869q from subreddit WritingPrompts\n",
      "Collected 134 comments from post 1gp869q\n",
      "Processing post 1grfdd4 from subreddit WritingPrompts\n",
      "Collected 136 comments from post 1grfdd4\n",
      "Processing post 1grlwxb from subreddit WritingPrompts\n",
      "Collected 138 comments from post 1grlwxb\n",
      "Processing post 1grigoz from subreddit WritingPrompts\n",
      "Collected 140 comments from post 1grigoz\n",
      "Processing post 1gkjas4 from subreddit ExplainLikeImFive\n",
      "Collected 142 comments from post 1gkjas4\n",
      "Processing post 1grh3fo from subreddit ExplainLikeImFive\n",
      "Collected 144 comments from post 1grh3fo\n",
      "Processing post 1griedl from subreddit ExplainLikeImFive\n",
      "Collected 146 comments from post 1griedl\n",
      "Processing post 1gret9a from subreddit ExplainLikeImFive\n",
      "Collected 148 comments from post 1gret9a\n",
      "Processing post 18rtwhw from subreddit TrueOffMyChest\n",
      "Collected 150 comments from post 18rtwhw\n",
      "Processing post 1gl2ugq from subreddit TrueOffMyChest\n",
      "Collected 151 comments from post 1gl2ugq\n",
      "Processing post 1grh49o from subreddit TrueOffMyChest\n",
      "Collected 153 comments from post 1grh49o\n",
      "Processing post 1griegb from subreddit TrueOffMyChest\n",
      "Collected 155 comments from post 1griegb\n",
      "Processing post 1giya84 from subreddit UnpopularOpinion\n",
      "Collected 157 comments from post 1giya84\n",
      "Processing post 1gqk787 from subreddit UnpopularOpinion\n",
      "Collected 158 comments from post 1gqk787\n",
      "Processing post 1grmxbf from subreddit UnpopularOpinion\n",
      "Collected 160 comments from post 1grmxbf\n",
      "Processing post 1gre8sb from subreddit UnpopularOpinion\n",
      "Collected 162 comments from post 1gre8sb\n",
      "Processing post 1ge6159 from subreddit relationships\n",
      "Collected 162 comments from post 1ge6159\n",
      "Processing post 1grjoem from subreddit relationships\n",
      "Collected 164 comments from post 1grjoem\n",
      "Processing post 1gqzi5n from subreddit relationships\n",
      "Collected 166 comments from post 1gqzi5n\n",
      "Processing post 1grqb2w from subreddit relationships\n",
      "Collected 168 comments from post 1grqb2w\n",
      "Processing post 1gqcrb2 from subreddit TrueAskReddit\n",
      "Collected 170 comments from post 1gqcrb2\n",
      "Processing post 1gphr3y from subreddit TrueAskReddit\n",
      "Collected 172 comments from post 1gphr3y\n",
      "Processing post 1gq9gv8 from subreddit TrueAskReddit\n",
      "Collected 174 comments from post 1gq9gv8\n",
      "Processing post 1gnzeee from subreddit TrueAskReddit\n",
      "Collected 176 comments from post 1gnzeee\n",
      "Processing post 126xkib from subreddit Confession\n",
      "Collected 176 comments from post 126xkib\n",
      "Processing post 1gri0fd from subreddit Confession\n",
      "Collected 178 comments from post 1gri0fd\n",
      "Processing post 1grlgwf from subreddit Confession\n",
      "Collected 180 comments from post 1grlgwf\n",
      "Processing post 1grkl9l from subreddit Confession\n",
      "Collected 182 comments from post 1grkl9l\n",
      "Processing post q6cw0j from subreddit ShortScaryStories\n",
      "Collected 184 comments from post q6cw0j\n",
      "Processing post 1grm00n from subreddit ShortScaryStories\n",
      "Collected 186 comments from post 1grm00n\n",
      "Processing post 1grdw31 from subreddit ShortScaryStories\n",
      "Collected 188 comments from post 1grdw31\n",
      "Processing post 1gr9bbj from subreddit ShortScaryStories\n",
      "Collected 190 comments from post 1gr9bbj\n",
      "Processing post 1gm2riq from subreddit ProRevenge\n",
      "Collected 192 comments from post 1gm2riq\n",
      "Processing post 1g13tpw from subreddit ProRevenge\n",
      "Collected 194 comments from post 1g13tpw\n",
      "Processing post 1fova4p from subreddit ProRevenge\n",
      "Collected 196 comments from post 1fova4p\n",
      "Processing post 1exnr27 from subreddit ProRevenge\n",
      "Collected 198 comments from post 1exnr27\n",
      "Processing post unlvd0 from subreddit NuclearRevenge\n",
      "Collected 199 comments from post unlvd0\n",
      "Processing post 163v9oo from subreddit NuclearRevenge\n",
      "Collected 201 comments from post 163v9oo\n",
      "Processing post 1ftajvn from subreddit NuclearRevenge\n",
      "Collected 203 comments from post 1ftajvn\n",
      "Processing post 1fn6ndp from subreddit NuclearRevenge\n",
      "Collected 205 comments from post 1fn6ndp\n",
      "Processing post 1gmsp7q from subreddit LifeProTips\n",
      "Collected 207 comments from post 1gmsp7q\n",
      "Processing post 1grnpac from subreddit LifeProTips\n",
      "Collected 209 comments from post 1grnpac\n",
      "Processing post 1grd7pm from subreddit LifeProTips\n",
      "Collected 211 comments from post 1grd7pm\n",
      "Processing post 1grld19 from subreddit LifeProTips\n",
      "Collected 213 comments from post 1grld19\n",
      "Processing post 1aqgnxa from subreddit needadvice\n",
      "Collected 215 comments from post 1aqgnxa\n",
      "Processing post 1grqyin from subreddit needadvice\n",
      "Collected 216 comments from post 1grqyin\n",
      "Processing post 1gr64v4 from subreddit needadvice\n",
      "Collected 218 comments from post 1gr64v4\n",
      "Processing post 1gr1zio from subreddit needadvice\n",
      "Collected 220 comments from post 1gr1zio\n",
      "Processing post 1g9ojlt from subreddit TrueUnpopularOpinion\n",
      "Collected 222 comments from post 1g9ojlt\n",
      "Processing post ncm4ou from subreddit TrueUnpopularOpinion\n",
      "Collected 224 comments from post ncm4ou\n",
      "Processing post 1grecxd from subreddit TrueUnpopularOpinion\n",
      "Collected 226 comments from post 1grecxd\n",
      "Processing post 1grd7au from subreddit TrueUnpopularOpinion\n",
      "Collected 228 comments from post 1grd7au\n",
      "\n",
      "Comments saved to collected_comments_with_author_and_karma.csv\n",
      "\n",
      "Updated data with engagement levels saved to labeled_subreddit_posts.csv\n",
      "\n",
      "Displaying updated dataframe with engagement scores:\n",
      "      subreddit                                              title  \\\n",
      "0     AskReddit                2024 united states elections thread   \n",
      "1  ChangeMyView  meta: research collaboration opportunity with ...   \n",
      "2  ChangeMyView  cmv: goodhearted \"cultural appropriation\" is f...   \n",
      "3  ChangeMyView  cmv: anyone given a life sentence should also ...   \n",
      "4  ChangeMyView  cmv: most people are too lazy to actually seek...   \n",
      "\n",
      "                                             content  upvotes  upvote_ratio  \\\n",
      "0  please use this thread to discuss the ongoing ...      122          0.70   \n",
      "1  from time to time, cmv will partner with profe...        7          0.71   \n",
      "2  i am austrian and when non-austrians find a li...      132          0.85   \n",
      "3  anyone given a life sentence should also be gi...       45          0.84   \n",
      "4  i always see people online talking about how t...      123          0.84   \n",
      "\n",
      "   comments_count               author            timestamp  post_id  \\\n",
      "0            7415  AskRedditModerators  2024-11-05 17:14:10  1gkk9s3   \n",
      "1               9              Ansuz07  2024-10-30 10:42:19  1gfpl4m   \n",
      "2              69  Possible_Lemon_9527  2024-11-14 19:30:43  1grkyyp   \n",
      "3              38     Christ-The-Slave  2024-11-14 19:39:44  1grl5em   \n",
      "4              61   Neckties-Over-Bows  2024-11-14 12:39:25  1grbu7n   \n",
      "\n",
      "         date   time AM_PM  upvote_ratio_normalized  \\\n",
      "0  2024-11-05  05:14    PM                 0.684211   \n",
      "1  2024-10-30  10:42    AM                 0.694737   \n",
      "2  2024-11-14  07:30    PM                 0.842105   \n",
      "3  2024-11-14  07:39    PM                 0.831579   \n",
      "4  2024-11-14  12:39    PM                 0.831579   \n",
      "\n",
      "   comments_count_normalized  upvotes_normalized subreddit_name  \\\n",
      "0                   0.443322            0.004664      AskReddit   \n",
      "1                   0.000538            0.000268   ChangeMyView   \n",
      "2                   0.004125            0.005046   ChangeMyView   \n",
      "3                   0.002272            0.001720   ChangeMyView   \n",
      "4                   0.003647            0.004702   ChangeMyView   \n",
      "\n",
      "   subscriber_count  engagement  normalized_engagement engagement_level  \n",
      "0        49127302.0    0.000153               1.534177           Medium  \n",
      "1         3740187.0    0.000004               0.042779              Low  \n",
      "2         3740187.0    0.000054               0.537406              Low  \n",
      "3         3740187.0    0.000022               0.221914              Low  \n",
      "4         3740187.0    0.000049               0.491954              Low  \n"
     ]
    }
   ],
   "source": [
    "# Collect posts from all subreddits\n",
    "all_posts_df = collect_posts_from_subreddits(subreddit_list, post_limit=4)\n",
    "\n",
    "# Save all collected posts to a single CSV file\n",
    "csv_filename = \"subreddit_posts.csv\"\n",
    "all_posts_df.to_csv(csv_filename, index=False)\n",
    "print(f\"\\nAll posts data saved to {csv_filename}\")\n",
    "\n",
    "# Print summary of collected posts\n",
    "print(\"\\nSummary of collected posts:\")\n",
    "print(all_posts_df['subreddit'].value_counts())\n",
    "\n",
    "# Collect subreddit-level data (e.g., rules, moderation, etc.)\n",
    "subreddit_level_data = collect_subreddit_level_data(reddit, subreddit_list, limit=20)\n",
    "\n",
    "# Convert to DataFrame and save to CSV\n",
    "subreddit_level_df = pd.DataFrame(subreddit_level_data)\n",
    "subreddit_level_df.to_csv(\"subreddit_level_data.csv\", index=False)\n",
    "print(\"\\nSubreddit-level data saved to subreddit_level_data.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# Save updated posts data with engagement scores to CSV\n",
    "all_posts_df.to_csv(\"subreddit_posts_with_engagement.csv\", index=False)\n",
    "print(\"\\nSubreddit posts with engagement scores saved to subreddit_posts_with_engagement.csv\")\n",
    "\n",
    "# Run the function to collect top-level comments and save them to a new CSV\n",
    "save_comments_to_csv(\"subreddit_posts.csv\", comment_limit=2)\n",
    "\n",
    "cleaned_labeled_data = pd.read_csv(\"cleaned_labeled_subreddit_posts.csv\")\n",
    "cleaned_labeled_data = cleaned_labeled_data.merge(subreddit_level_df[['subreddit_name', 'subscriber_count']], \n",
    "                                                  left_on='subreddit', right_on='subreddit_name', how='left')\n",
    "\n",
    "cleaned_labeled_data['engagement'] = cleaned_labeled_data.apply(calculate_engagement, axis=1)\n",
    "cleaned_labeled_data['normalized_engagement'] = cleaned_labeled_data['engagement'] * 10000\n",
    "\n",
    "cleaned_labeled_data = define_engagement(cleaned_labeled_data)\n",
    "\n",
    "cleaned_labeled_data.to_csv(\"labeled_subreddit_posts.csv\", index=False)\n",
    "print(\"\\nUpdated data with engagement levels saved to labeled_subreddit_posts.csv\")\n",
    "\n",
    "print(\"\\nDisplaying updated dataframe with engagement scores:\")\n",
    "print(cleaned_labeled_data.head())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL CLEANED CSV Creation and Storing\n",
    "In this Code Block we will be creating a csv file and storing all our scraped data."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
