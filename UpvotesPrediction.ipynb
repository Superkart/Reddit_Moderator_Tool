{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Cleaning Data From CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       subreddit                                              title  \\\n",
      "0      AskReddit                2024 united states elections thread   \n",
      "1   ChangeMyView  meta: research collaboration opportunity with ...   \n",
      "2   ChangeMyView  cmv: goodhearted \"cultural appropriation\" is f...   \n",
      "3   ChangeMyView  cmv: anyone given a life sentence should also ...   \n",
      "4   ChangeMyView  cmv: most people are too lazy to actually seek...   \n",
      "5   ChangeMyView  cmv: it isn't hyperbolic for the west to be wo...   \n",
      "6   ChangeMyView  cmv: the american healthcare system is not onl...   \n",
      "7   ChangeMyView  cmv: the best way to tax corporations would be...   \n",
      "8   ChangeMyView  cmv: the period of time when women were joking...   \n",
      "9   ChangeMyView  cmv: the american led world system is not good...   \n",
      "10  ChangeMyView       cmv:  the promise to gorbachev has no value.   \n",
      "11  ChangeMyView  cmv: religion just a cycle of generational bra...   \n",
      "12  ChangeMyView  cmv: you should be able to shoot to kill anyon...   \n",
      "13  ChangeMyView  cmv: the current state of israel is giving peo...   \n",
      "14  ChangeMyView  cmv: the roosevelts are incredibly overrated p...   \n",
      "15  ChangeMyView  cmv: cargo pants should never have pockets bel...   \n",
      "16  ChangeMyView  cmv: women withholding healthcare information ...   \n",
      "17  ChangeMyView  cmv: without other-wordly knowledge, values ar...   \n",
      "18  ChangeMyView  cmv: the “girl-bossification” of sex work is n...   \n",
      "19  ChangeMyView  cmv: piracy is stealing, is morally wrong, and...   \n",
      "\n",
      "                                              content  upvotes  upvote_ratio  \\\n",
      "0   please use this thread to discuss the ongoing ...      122          0.70   \n",
      "1   from time to time, cmv will partner with profe...        7          0.71   \n",
      "2   i am austrian and when non-austrians find a li...      132          0.85   \n",
      "3   anyone given a life sentence should also be gi...       45          0.84   \n",
      "4   i always see people online talking about how t...      123          0.84   \n",
      "5   like clockwork, economic downturn inevitably l...      425          0.69   \n",
      "6   first of all, uhc would cost significantly les...     1696          0.83   \n",
      "7   the boiling frog in the us is the middle class...        5          0.58   \n",
      "8   i’m saying this as a leftist myself.\\n\\ni beli...       30          0.51   \n",
      "9   the american led world system is not good, the...      141          0.63   \n",
      "10  they were talks ,full of ifs and woulds and ma...       49          0.73   \n",
      "11  i’ve been thinking a lot about how religion is...       97          0.59   \n",
      "12  shower thought: self defence is not recognized...        0          0.46   \n",
      "13  ever since the war between israel and palestin...      233          0.55   \n",
      "14  to be clear, i'm specifically talking about th...        0          0.38   \n",
      "15  cargo pants are pants with external pockets on...        0          0.48   \n",
      "16  there are so many women online aggressively pu...        0          0.07   \n",
      "17  when i was around 14-16 i resolved a lot of th...        0          0.36   \n",
      "18  it has become apparent in recent times that se...      397          0.80   \n",
      "19  to clarify, i’m referring to piracy in terms o...        0          0.28   \n",
      "\n",
      "    comments_count                author            timestamp  post_id  \\\n",
      "0             7415   AskRedditModerators  2024-11-05 17:14:10  1gkk9s3   \n",
      "1                9               Ansuz07  2024-10-30 10:42:19  1gfpl4m   \n",
      "2               69   Possible_Lemon_9527  2024-11-14 19:30:43  1grkyyp   \n",
      "3               38      Christ-The-Slave  2024-11-14 19:39:44  1grl5em   \n",
      "4               61    Neckties-Over-Bows  2024-11-14 12:39:25  1grbu7n   \n",
      "5              537          hotdog_jones  2024-11-14 04:24:10  1gr1tbe   \n",
      "6             1062             4K05H4784  2024-11-13 13:16:12  1gqky67   \n",
      "7               22          megadelegate  2024-11-14 20:35:21  1grm7tk   \n",
      "8              959      Firm-General3739  2024-11-14 05:52:59  1gr32zs   \n",
      "9              258       ForgetfullRelms  2024-11-13 21:57:41  1gqw8lm   \n",
      "10              57        MattTerminator  2024-11-14 02:54:48  1gr0nv6   \n",
      "11             254       Elegant_Hat5101  2024-11-13 18:54:31  1gqsofs   \n",
      "12              84   FarConstruction4877  2024-11-14 19:49:14  1grlc0m   \n",
      "13            1437             MrsKebabs  2024-11-13 09:17:07  1gqf6ey   \n",
      "14              35    maybemorningstar69  2024-11-14 17:49:17  1griwky   \n",
      "15              61  Parking-Special-3965  2024-11-14 11:06:53  1gr9m3u   \n",
      "16               8              CytoPath  2024-11-14 20:24:46  1grm0ka   \n",
      "17              94         AtomAndAether  2024-11-14 11:58:39  1grauzi   \n",
      "18             494     Careful-Panda9885  2024-11-12 12:28:52  1gprni8   \n",
      "19              76       Then_Cable_8908  2024-11-14 11:51:28  1graorr   \n",
      "\n",
      "          date   time AM_PM  upvote_ratio_normalized  \\\n",
      "0   2024-11-05  05:14    PM                 0.684211   \n",
      "1   2024-10-30  10:42    AM                 0.694737   \n",
      "2   2024-11-14  07:30    PM                 0.842105   \n",
      "3   2024-11-14  07:39    PM                 0.831579   \n",
      "4   2024-11-14  12:39    PM                 0.831579   \n",
      "5   2024-11-14  04:24    AM                 0.673684   \n",
      "6   2024-11-13  01:16    PM                 0.821053   \n",
      "7   2024-11-14  08:35    PM                 0.557895   \n",
      "8   2024-11-14  05:52    AM                 0.484211   \n",
      "9   2024-11-13  09:57    PM                 0.610526   \n",
      "10  2024-11-14  02:54    AM                 0.715789   \n",
      "11  2024-11-13  06:54    PM                 0.568421   \n",
      "12  2024-11-14  07:49    PM                 0.431579   \n",
      "13  2024-11-13  09:17    AM                 0.526316   \n",
      "14  2024-11-14  05:49    PM                 0.347368   \n",
      "15  2024-11-14  11:06    AM                 0.452632   \n",
      "16  2024-11-14  08:24    PM                 0.021053   \n",
      "17  2024-11-14  11:58    AM                 0.326316   \n",
      "18  2024-11-12  12:28    PM                 0.789474   \n",
      "19  2024-11-14  11:51    AM                 0.242105   \n",
      "\n",
      "    comments_count_normalized  upvotes_normalized subreddit_name  \\\n",
      "0                    0.443322            0.004664      AskReddit   \n",
      "1                    0.000538            0.000268   ChangeMyView   \n",
      "2                    0.004125            0.005046   ChangeMyView   \n",
      "3                    0.002272            0.001720   ChangeMyView   \n",
      "4                    0.003647            0.004702   ChangeMyView   \n",
      "5                    0.032106            0.016247   ChangeMyView   \n",
      "6                    0.063494            0.064837   ChangeMyView   \n",
      "7                    0.001315            0.000191   ChangeMyView   \n",
      "8                    0.057336            0.001147   ChangeMyView   \n",
      "9                    0.015425            0.005390   ChangeMyView   \n",
      "10                   0.003408            0.001873   ChangeMyView   \n",
      "11                   0.015186            0.003708   ChangeMyView   \n",
      "12                   0.005022            0.000000   ChangeMyView   \n",
      "13                   0.085914            0.008907   ChangeMyView   \n",
      "14                   0.002093            0.000000   ChangeMyView   \n",
      "15                   0.003647            0.000000   ChangeMyView   \n",
      "16                   0.000478            0.000000   ChangeMyView   \n",
      "17                   0.005620            0.000000   ChangeMyView   \n",
      "18                   0.029535            0.015177   ChangeMyView   \n",
      "19                   0.004544            0.000000   ChangeMyView   \n",
      "\n",
      "    subscriber_count  engagement  normalized_engagement engagement_level  \n",
      "0         49338678.0    0.000153               1.527605           Medium  \n",
      "1          3745485.0    0.000004               0.042718              Low  \n",
      "2          3745485.0    0.000054               0.536646              Low  \n",
      "3          3745485.0    0.000022               0.221600              Low  \n",
      "4          3745485.0    0.000049               0.491258              Low  \n",
      "5          3745485.0    0.000257               2.568426           Medium  \n",
      "6          3745485.0    0.000736               7.363532             High  \n",
      "7          3745485.0    0.000007               0.072087              Low  \n",
      "8          3745485.0    0.000264               2.640513           Medium  \n",
      "9          3745485.0    0.000107               1.065283           Medium  \n",
      "10         3745485.0    0.000028               0.283007              Low  \n",
      "11         3745485.0    0.000094               0.937128              Low  \n",
      "12         3745485.0    0.000022               0.224270              Low  \n",
      "13         3745485.0    0.000446               4.458702           Medium  \n",
      "14         3745485.0    0.000009               0.093446              Low  \n",
      "15         3745485.0    0.000016               0.162863              Low  \n",
      "16         3745485.0    0.000002               0.021359              Low  \n",
      "17         3745485.0    0.000025               0.250969              Low  \n",
      "18         3745485.0    0.000238               2.378864           Medium  \n",
      "19         3745485.0    0.000020               0.202911              Low  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('labeled_subreddit_posts.csv')\n",
    "df = df.dropna()\n",
    "\n",
    "print(df.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   subreddit_label  upvote_ratio  comments_count  subscriber_count  hour  \\\n",
      "0                2      0.684211        0.443322          1.000000    17   \n",
      "1                5      0.694737        0.000538          0.054069    10   \n",
      "2                5      0.842105        0.004125          0.054069    19   \n",
      "3                5      0.831579        0.002272          0.054069    19   \n",
      "4                5      0.831579        0.003647          0.054069    12   \n",
      "\n",
      "   day_of_week  \n",
      "0            1  \n",
      "1            2  \n",
      "2            3  \n",
      "3            3  \n",
      "4            3  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['subreddit_label'] = le.fit_transform(df['subreddit_name'])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "normalized_columns = ['upvote_ratio', 'comments_count', 'subscriber_count', 'engagement']\n",
    "df[normalized_columns] = scaler.fit_transform(df[normalized_columns])\n",
    "\n",
    "df['hour'] = pd.to_datetime(df['timestamp']).dt.hour\n",
    "df['day_of_week'] = pd.to_datetime(df['timestamp']).dt.dayofweek\n",
    "\n",
    "print(df[['subreddit_label', 'upvote_ratio', 'comments_count', 'subscriber_count', 'hour', 'day_of_week']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Text Data Using TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of text features: (10344, 500)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Combine 'title' and 'content' for text analysis\n",
    "df['combined_text'] = df['title'] + \" \" + df['content']\n",
    "\n",
    "# Convert text to TF-IDF features\n",
    "tfidf = TfidfVectorizer(max_features=500)  # Limit to 500 features for efficiency\n",
    "text_features = tfidf.fit_transform(df['combined_text']).toarray()\n",
    "\n",
    "print(\"Shape of text features:\", text_features.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Features and Target Variable\n",
    "Combining all the processed features into a single matrix and extract the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (10344, 506)\n",
      "Target variable shape: (10344,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.hstack([\n",
    "    text_features,\n",
    "    df[['subreddit_label', 'upvote_ratio', 'comments_count', 'subscriber_count', 'hour', 'day_of_week']].values\n",
    "])\n",
    "\n",
    "y = df['upvotes']\n",
    "\n",
    "print(\"Feature matrix shape:\", X.shape)\n",
    "print(\"Target variable shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split\n",
    "Split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 7240\n",
      "Testing set size: 3104\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data: 70% training, 30% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Testing set size: {X_test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Regression Model\n",
    "Train a Random Forest Regressor Model to predict upvotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Model\n",
    "Evaluating the performance of the model using Mean Squared Error (MSE) and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 596277.0358574743\n",
      "R-squared: 0.1623106901506015\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Upvotes for a New Post\n",
    "Using the trained model to predict upvotes for a new Reddit post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: 'datascience'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32md:\\UICSem1\\IntroToDataScience\\class-project-cheesy-little-explorers\\venv\\Lib\\site-packages\\sklearn\\utils\\_encode.py:225\u001b[0m, in \u001b[0;36m_encode\u001b[1;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_map_to_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\UICSem1\\IntroToDataScience\\class-project-cheesy-little-explorers\\venv\\Lib\\site-packages\\sklearn\\utils\\_encode.py:165\u001b[0m, in \u001b[0;36m_map_to_integer\u001b[1;34m(values, uniques)\u001b[0m\n\u001b[0;32m    164\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[1;32md:\\UICSem1\\IntroToDataScience\\class-project-cheesy-little-explorers\\venv\\Lib\\site-packages\\sklearn\\utils\\_encode.py:159\u001b[0m, in \u001b[0;36m_nandict.__missing__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnan_value\n\u001b[1;32m--> 159\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'datascience'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Preprocess new post\u001b[39;00m\n\u001b[0;32m     14\u001b[0m new_combined_text \u001b[38;5;241m=\u001b[39m tfidf\u001b[38;5;241m.\u001b[39mtransform([new_post[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m new_post[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]])\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[1;32m---> 15\u001b[0m new_subreddit_label \u001b[38;5;241m=\u001b[39m \u001b[43mle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnew_post\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msubreddit_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Combine features\u001b[39;00m\n\u001b[0;32m     18\u001b[0m new_X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack([\n\u001b[0;32m     19\u001b[0m     new_combined_text,\n\u001b[0;32m     20\u001b[0m     np\u001b[38;5;241m.\u001b[39marray([[new_subreddit_label, new_post[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupvote_ratio\u001b[39m\u001b[38;5;124m'\u001b[39m], new_post[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomments_count\u001b[39m\u001b[38;5;124m'\u001b[39m], new_post[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubscriber_count\u001b[39m\u001b[38;5;124m'\u001b[39m], new_post[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhour\u001b[39m\u001b[38;5;124m'\u001b[39m], new_post[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday_of_week\u001b[39m\u001b[38;5;124m'\u001b[39m]]])\n\u001b[0;32m     21\u001b[0m ])\n",
      "File \u001b[1;32md:\\UICSem1\\IntroToDataScience\\class-project-cheesy-little-explorers\\venv\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:137\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\UICSem1\\IntroToDataScience\\class-project-cheesy-little-explorers\\venv\\Lib\\site-packages\\sklearn\\utils\\_encode.py:227\u001b[0m, in \u001b[0;36m_encode\u001b[1;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 227\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_unknown:\n",
      "\u001b[1;31mValueError\u001b[0m: y contains previously unseen labels: 'datascience'"
     ]
    }
   ],
   "source": [
    "# Example new post data\n",
    "new_post = {\n",
    "    'title': \"How to start with Data Science?\",\n",
    "    'content': \"I am looking for beginner-friendly resources to learn Data Science.\",\n",
    "    'subreddit_name': \"datascience\",\n",
    "    'upvote_ratio': 0.8,\n",
    "    'comments_count': 15,\n",
    "    'subscriber_count': 500000,\n",
    "    'hour': 14,\n",
    "    'day_of_week': 2\n",
    "}\n",
    "\n",
    "# Preprocess new post\n",
    "new_combined_text = tfidf.transform([new_post['title'] + \" \" + new_post['content']]).toarray()\n",
    "new_subreddit_label = le.transform([new_post['subreddit_name']])[0]\n",
    "\n",
    "# Combine features\n",
    "new_X = np.hstack([\n",
    "    new_combined_text,\n",
    "    np.array([[new_subreddit_label, new_post['upvote_ratio'], new_post['comments_count'], new_post['subscriber_count'], new_post['hour'], new_post['day_of_week']]])\n",
    "])\n",
    "\n",
    "# Predict upvotes\n",
    "predicted_upvotes = model.predict(new_X)\n",
    "print(f\"Predicted Upvotes: {int(predicted_upvotes[0])}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
